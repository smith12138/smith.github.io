{"meta":{"title":"清风明月","subtitle":"","description":"","author":"Ming Hui","url":"http://yoursite.com","root":"/"},"posts":[{"tags":[],"title":"验证码识别方法","date":"2020/08/09","text":"编码实现（ tesseract-ocr ） 这是 google 开源的一款识别工具，最早是用来识别文字的。 识别率低 不建议自己去开发，因为识别各种验证码，是需要大量时间的。 而且验证码一旦更改了，代码就无效了 在线打码 识别率高达90%以上，速度较快 依靠代码识别技术，平台提供 API (超级鹰) 人工打码 字面意思，人工识别，所以识别率最高，成本也高","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"},{"name":"Selenium","slug":"Selenium","permalink":"http://yoursite.com/tags/Selenium/"}],"title":"解决倒立文字验证","date":"2020/08/09","text":"引用第三方库，调用接口进行识别安装 zheye 第三方库git clone https://github.com/996refuse/zheye.git 安装依赖包，并移动到项目目录下其中 tensorflow 库，改为1.13版本接口调用示例：from zheye.zheye import zheye z = zheye() positions = z.Recognize('zhihu_image/a.gif') print(positions)","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/%E8%AF%86%E5%88%AB%E5%80%92%E7%AB%8B%E6%96%87%E5%AD%97/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"},{"name":"Selenium","slug":"Selenium","permalink":"http://yoursite.com/tags/Selenium/"}],"title":"模拟 Chrome 知乎登录","date":"2020/08/09","text":"创建 chromedriver 实例class ZhihuReqSpider(scrapy.Spider): name = 'zhihu_req' allowed_domains = ['www.zhihu.com'] start_urls = ['http://www.zhihu.com/'] def parse(self, response): pass def start_requests(self): \"\"\" 在继承 Spider 的时候，入口方法是 start_requests 现在要爬取知乎，而且必须进行登录 第一步就是完成登录，所以必须要重写 start_request 方法 调用 chromedriver，模拟登录知乎 \"\"\" from selenium import webdriver from selenium.webdriver.chrome.options import Options from selenium.webdriver.common.keys import Keys chrome_option = Options() # 添加参数 --disable-extensions &lt;禁用扩展名> chrome_option.add_argument('--disable-extensions') # 添加实验选项 debuggerAddress &lt;调试器地址, 地址> chrome_option.add_experimental_option('debuggerAddress', '127.0.0.1:9222') # 传入 driver.exe 执行路径， 实例化对象 browser = webdriver.Chrome(executable_path='E:/Template/chromedriver.exe', chrome_options=chrome_option) 操控 HTML 元素try: # 最大化 browser window browser.maximize_window() # 如果窗口已经最大化的话，再调用最大化方法，会抛出异常 except: pass browser.get('https://www.zhihu.com/signin?next=%2F') browser.find_element_by_css_selector('div[class=\"SignFlow-tabs\"] div:nth-child(2)').click() browser.find_element_by_css_selector('.SignFlow-account input[name=\"username\"]').send_keys(Keys.CONTROL + 'a') browser.find_element_by_css_selector('.SignFlow-account input[name=\"username\"]').send_keys('17688718015') browser.find_element_by_css_selector('.SignFlow-password input[name=\"password\"]').send_keys(Keys.CONTROL + 'a') browser.find_element_by_css_selector('.SignFlow-password input[name=\"password\"]').send_keys('tanling.') browser.find_element_by_css_selector('form[class=\"SignFlow Login-content\"] button[type=\"submit\"]').click() time.sleep(10) 判断是否登录成功while login_success: try: # 寻找 HTML 数据中，是否包含 &lt;提醒> 图标元素 popover = browser.find_element_by_css_selector('.Popover') login_success = True except: pass 判断验证码类型try: # 寻找是否含有 英文验证码 english_captcha_element = browser.find_element_by_css_selector('.Captcha-englishImg') except: english_captcha_element = None try: # 寻找是否含有 中文验证码 chinese_captcha_element = browser.find_element_by_css_selector('.Captcha-chineseImg') except: chinese_captcha_element = None 获取中文验证码 image，在可渲染画布中的位置 ，用作Click -&gt; 注意HTML缩放 size == 100%if chinese_captcha_element: # 使用 selenium 中 location 方法，获取元素在可渲染画布中的位置（browser-地址栏位置除外） ele_postion = chinese_captcha_element.location x_relative = ele_postion['x'] y_relative = ele_postion['y'] 保存验证码 image ，并调用第三方库识别# 使用 outerHeight - innerHeight，得到导航栏的高度（包括文件下载导航栏） browser_navigation_panel_height = browser.execute_script( 'return window.outerHeight - window.innerHeight;' ) # 稳妥方法：固定70像素 browser_navigation_panel_height = 70 # 获取验证码元素 -> src 属性 base64_text = chinese_captcha_element.get_attribute('src') # 这个空字符串与 base64 的编码不一样，直接将它保存，文件是存在问题的，会多出编码：%0A &lt;后续验证，貌似并没有多出> code = base64_text.replace('data:image/jpg;base64,', '').replace('%0A', '') # 以二进制形式，打开文件，进行 base64 编码，写入数据 with open('yzm_zh.jpeg', 'wb') as fp: fp.write(base64.b64decode(code)) from zheye.zheye import zheye z = zheye() positions = z.Recognize('yzm_zh.jpeg') # 适配 HTML 中的 size positions = [[int(key / 2) for key in position] for position in positions] # 对换 x,y 轴坐标，文字坐标 + 页面位置 + 导航栏位置 positions = [[position[1] + x_relative, position[0] + browser_navigation_panel_height + y_relative] for position in positions] # 点击验证码倒立文字 [[move(position[0], position[1]), click()] for position in positions] 获取英文验证码image，并保存imageif english_captcha_element: # 获取验证码元素 -> src 属性 base64_text = english_captcha_element.get_attribute('src') # 这个空字符串与 base64 的编码不一样，直接将它保存，文件是存在问题的，会多出编码： # %0A &lt;后续验证，貌似并没有多出> code = base64_text.replace('data:image/jpg;base64,', '').replace('%0A', '') # 以base64.b64decode 形式写入文件 with open('yzm_zh.jpeg', 'wb') as fp: fp.write(base64.b64decode(code)) 调用第三方API，识别 captcha Image 并 Input response captchafrom Tools.chaojiying import ChaojiyingClient identify: [ChaojiyingClient] = ChaojiyingClient('willsmith', 'tanling.', '905454') # 以 bytes 数据，进行赋值 img = open('yzm_zh.jpeg', 'rb').read() # 避免获取验证码失败 result = '' while True: if not result: result = identify.post_pic(img, 1902) else: break browser.find_element_by_css_selector('.Input-wrapper input[name=\"captcha\"]').send_keys(Keys.CONTROL + 'a') browser.find_element_by_css_selector('.Input-wrapper input[name=\"captcha\"]').send_keys(result['pic_str']) 避免账号，密码被清空，防范措施# 避免账号密码被清空 if not login_success: browser.find_element_by_css_selector('div[class=\"SignFlow-tabs\"] div:nth-child(2)').click() browser.find_element_by_css_selector('.SignFlow-account input[name=\"username\"]').send_keys( Keys.CONTROL + 'a') browser.find_element_by_css_selector('.SignFlow-account input[name=\"username\"]').send_keys( '17688718015') browser.find_element_by_css_selector('.SignFlow-password input[name=\"password\"]').send_keys( Keys.CONTROL + 'a') browser.find_element_by_css_selector('.SignFlow-password input[name=\"password\"]').send_keys('tanling.') browser.find_element_by_css_selector( 'form[class=\"SignFlow Login-content\"] button[type=\"submit\"]').click() 保存 cookie，并调用Request 传入 参数，以便后续 parse data# 保存cookie cookies = browser.get_cookies() pickle.dump(cookies, open('E:\\WellHome\\ArticleSpider\\cookie\\zhihu.cookie', 'wb')) cookies_dict = DataConvert.merge_dicts([{cookie['name']: cookie['value']} for cookie in cookies]) # Scrapy内置了重复过滤功能，默认情况下该功能处于打开状态。 return [scrapy.Request(url=ZhihuFormalFirefoxSpider.start_urls[0], dont_filter=True, cookies=cookies_dict)]","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/Selenium_Chrome%E6%A8%A1%E6%8B%9F%E7%9F%A5%E4%B9%8E%E7%99%BB%E5%BD%95/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"},{"name":"Selenium","slug":"Selenium","permalink":"http://yoursite.com/tags/Selenium/"}],"title":"模拟 Firefox 知乎登录","date":"2020/08/09","text":"代码示例： # -*- coding: utf-8 -*- from ArticleSpider.utils.common import DataConvert import scrapy import pickle from mouse import move, click from selenium import webdriver from selenium.webdriver import DesiredCapabilities from selenium.webdriver.common.keys import Keys # 拼接域名与url, 并兼容py2 try: import urlparse as parse except: from urllib import parse import time import base64 class ZhihuFormalFirefoxSpider(scrapy.Spider): name = 'zhihu_formal_firefox' allowed_domains = ['www.zhihu.com'] start_urls = ['http://www.zhihu.com/'] def parse(self, response): \"\"\" 提取HTML中所有的URL，并跟踪这些URL进一步爬取 如果提取的URL中包含 question/xxx，就下载之后交给解析方法 \"\"\" # 提取所有 a 标签中的 href 属性参数 all_urls = response.css('a::attr(href)').extract() # 拼接 url 与 域名 all_urls = [parse.urljoin(response.url, url) for url in all_urls] for url in all_urls: pass def start_requests(self): \"\"\" 在继承 Spider 的时候，入口方法是 start_requests 现在要爬取知乎，而且必须进行登录 第一步就是完成登录，所以必须要重写 start_request 方法 调用 firefox_driver，模拟登录知乎 \"\"\" convert = DataConvert() profile = webdriver.FirefoxProfile( r'C:\\Users\\WIN10\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\1o72doa9.default-release' # 'C:/Users/A17/AppData/Roaming/Mozilla/Firefox/Profiles/x3sshdnj.default-release' ) # 判断 webdriver 已启动 : 关闭 profile.set_preference(\"dom.webdriver.enabled\", False) # 使用自动化扩展 : 关闭 profile.set_preference('useAutomationExtension', False) profile.update_preferences() desired = DesiredCapabilities.FIREFOX browser = webdriver.Firefox(executable_path='E:/Template/geckodriver.exe', firefox_profile=profile, desired_capabilities=desired, service_log_path='Log/geckodriver.log') try: # 最大化 browser window browser.maximize_window() # 如果窗口已经最大化的话，再调用最大化方法，会抛出异常 except: pass browser.get('https://www.zhihu.com/signin?next=%2F') browser.find_element_by_css_selector('div[class=\"SignFlow-tabs\"] div:nth-child(2)').click() browser.find_element_by_css_selector('.SignFlow-account input[name=\"username\"]').send_keys(Keys.CONTROL + 'a') browser.find_element_by_css_selector('.SignFlow-account input[name=\"username\"]').send_keys('17688718015') browser.find_element_by_css_selector('.SignFlow-password input[name=\"password\"]').send_keys(Keys.CONTROL + 'a') browser.find_element_by_css_selector('.SignFlow-password input[name=\"password\"]').send_keys('tanling.') browser.find_element_by_css_selector('form[class=\"SignFlow Login-content\"] button[type=\"submit\"]').click() time.sleep(10) # 有可能登录失败 login_success = False while not login_success: # 寻找 HTML 数据中，是否包含 &lt;提醒> 图标元素 try: popover = browser.find_element_by_css_selector('.Popover') login_success = True except: pass # 寻找英文验证码 try: # 寻找是否含有 英文验证码 english_captcha_element = browser.find_element_by_css_selector('.Captcha-englishImg') except: english_captcha_element = None # 寻找中文验证码 try: # 寻找是否含有 中文验证码 chinese_captcha_element = browser.find_element_by_css_selector('.Captcha-chineseImg') except: chinese_captcha_element = None # 中文倒立文字识别登录 if chinese_captcha_element: # 使用 selenium 中 location 方法，获取元素在可渲染画布中的位置（browser-地址栏位置除外） ele_position = chinese_captcha_element.location x_relative = ele_position['x'] y_relative = ele_position['y'] # 使用 outerHeight - innerHeight，得到导航栏的高度（包括文件下载导航栏） # browser_navigation_panel_height = browser.execute_script( # 'return window.outerHeight - window.innerHeight;' # ) # 稳妥方法：固定70 browser_navigation_panel_height = 100 # 获取验证码元素 -> src 属性 base64_text = chinese_captcha_element.get_attribute('src') positions = convert.check_chinese_captcha(base64_text) # 适配 HTML 中的 size positions = [[int(key / 2) for key in position] for position in positions] # 对换 x,y 轴坐标，文字坐标 + 页面位置 + 导航栏位置 positions = [[position[1] + x_relative, position[0] + browser_navigation_panel_height + y_relative] for position in positions] # 点击验证码倒立文字 [[move(position[0], position[1]), click()] for position in positions] if english_captcha_element: # 获取验证码元素 -> src 属性 base64_text = english_captcha_element.get_attribute('src') result = convert.check_english_captcha(base64_text) browser.find_element_by_css_selector('.Input-wrapper input[name=\"captcha\"]').send_keys(Keys.CONTROL + 'a') browser.find_element_by_css_selector('.Input-wrapper input[name=\"captcha\"]').send_keys(result['pic_str']) # 避免账号密码被清空 if not login_success: browser.find_element_by_css_selector('div[class=\"SignFlow-tabs\"] div:nth-child(2)').click() browser.find_element_by_css_selector('.SignFlow-account input[name=\"username\"]').send_keys( Keys.CONTROL + 'a') browser.find_element_by_css_selector('.SignFlow-account input[name=\"username\"]').send_keys( '17688718015') browser.find_element_by_css_selector('.SignFlow-password input[name=\"password\"]').send_keys( Keys.CONTROL + 'a') browser.find_element_by_css_selector('.SignFlow-password input[name=\"password\"]').send_keys('tanling.') browser.find_element_by_css_selector( 'form[class=\"SignFlow Login-content\"] button[type=\"submit\"]').click() # 保存cookie cookies = browser.get_cookies() pickle.dump(cookies, open('E:\\WellHome\\ArticleSpider\\cookie\\zhihu.cookie', 'wb')) cookies_dict = DataConvert.merge_dicts([{cookie['name']: cookie['value']} for cookie in cookies]) # Scrapy 内置了重复过滤功能 dont_filter，默认情况下该功能处于打开状态。没有进行callback，所以会默认进入parse方法中 return [scrapy.Request(url=ZhihuFormalFirefoxSpider.start_urls[0], dont_filter=True, cookies=cookies_dict)]","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/selenium_firefox%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"Selenium 模拟微博登录","date":"2020/08/09","text":"启用 ChromeDriverdef start_requests(self): \"\"\"使用 selenium 获取 cookie\"\"\" # 获得 Options(操作)对象 chrome_option = Options() # 添加到实例私有属性 self._arguments(论据)（list）, 作为后续 webdriver 设置启动选项 chrome_option.add_argument('--disable-extensions') # 禁用扩展 # 添加到实例私有属性 self._experimental_options(实验选择)（dict）, 第一个参数为 key, 第二个参数为 value进行赋值 # 也是作为后续 webdriver 启动设置选项 chrome_option.add_experimental_option('debuggerAddress', '127.0.0.1:9222') # 调试设置 # 获取当前目录相对路径 string item_dir = os.path.dirname(os.path.abspath(__file__)) # 打开 webdriver 启用配置 with open(item_dir + r'/setting.conf') as f: exe_key = f.read() with open(item_dir + r'/executable_path.conf') as f: exe_path = json.loads(f.read())['chrome'][exe_key] # 传入可执行文件绝对路径, 此前 options 实例, 生成 browser 实例 browser = webdriver.Chrome(executable_path=exe_path[0], chrome_options=chrome_option) # 调用实例 get 进入对应 web 页面 browser.get(self.start_urls[0]) # 放大 browser window try: browser.maximize_window() except: pass 模拟登录# 避免输入框存在未知字符 browser.find_element_by_css_selector('input[id=\"loginname\"]').send_keys(Keys.CONTROL + 'a') # 定位到账号输入框, 输入账号 browser.find_element_by_css_selector('input[id=\"loginname\"]').send_keys(WEIBO_CONFIG['account']) # 避免输入框存在未知字符 browser.find_element_by_css_selector('input[type=\"password\"]').send_keys(Keys.CONTROL + 'a') # 定位到密码输入框, 输入密码 browser.find_element_by_css_selector('input[type=\"password\"]').send_keys(WEIBO_CONFIG['password']) # 定位到登录按钮, 触发点击事件 browser.find_element_by_css_selector('div[node-type=\"normal_form\"] a[node-type=\"submitBtn\"]').click() 验证码识别针对动态验证码，处理验证码识别 截图当前验证码# 给定循环条件 login_status = False while not login_status: try: # 避免 web 页面响应时间过慢 time.sleep(10) # 定位登录成功后 web 页面中的特定元素 browser.find_element_by_css_selector('em[class=\"W_ficon ficon_mail S_ficon\"]') # 已找到, 改变条件变量 login_status = True # 跳出循环 break # 未找到登录成功页面指定元素 except: pass try: # 设置验证码图片, 保存相对路径 captcha_code = BASE_DIR + r'\\image\\captcha_code.png' # 设置样本图片, 相对路径 check_image = BASE_DIR + r'\\image\\check_image.png' # 寻找是否弹出验证码图片 captcha_element = browser.find_element_by_css_selector('img[node-type=\"verifycode_image\"]') # 获取元素属性 src captcha_url = captcha_element.get_attribute('src') # 判断是否包含 http / https (判断是否为 url) if 'http' in captcha_url or 'https' in captcha_url: # 引入第三方库, pyautogui(主要使用其截图功能) import pyautogui # 传入定位图片路径(string), 若定位成功可获取实例对象属性; # Box(left=1111, top=111, width=33, height=33), 可以下标索引/关键字取出 point_coords = pyautogui.locateOnScreen(check_image) try: # 取出 x, y, width, height point_coord = [point_coords.left, point_coords.top, point_coords.width, point_coords.height] except KeyError as e: print('not found : {} . error:{}'.format(check_image, e)) try: # 验证码修正左侧，顶部，宽度和高度 px correct = [120, -90, 30, 0] # 调用 zip 函数, 列表推导式方式, 对位相加数组中的元素, 获得修正后的坐标 point_coord = tuple([px + p for px, p in zip(correct, point_coord)]) # 调用 screenshot 入口, 传入图片保存路径, region=坐标 pyautogui.screenshot(captcha_code, region=point_coord) print('captcha code image screenshot success!!!!!') except Exception as e: print(str(e)) 调用第三方服务，识别验证码# 调用 exists 函数, 传入路径, 判断验证码图片是否存在 if os.path.exists(captcha_code): # 实例化自定义工具类 DC = DataConvert() # 取出保存验证码, 字节二进制数据 image_data = open(captcha_code, 'rb').read() # 调用工具实例 -> 英文数字验证码识别方法, 传入二进制数据, 取出 pic_str key 的 value captcha_result = DC.check_english_captcha(image_data)['pic_str'] print('receive code : &lt;{}>'.format(captcha_result)) # 定位到验证码输入框(因为是JS动态加载的, 不点击不会改变), 触发点击事件 browser.find_element_by_css_selector('input[name=\"verifycode\"]').click() print('----click input----') # 定位到验证码输入框, 输入验证码 browser.find_element_by_css_selector('input[class=\"W_input W_input_focus\"]' ).send_keys(captcha_result) print('-----input captcha code success------') # 点击登录按钮 browser.find_element_by_css_selector( 'div[node-type=\"normal_form\"] a[class=\"W_btn_a btn_32px \"]').click() print('----click login button-----') # 调用系统处理模块, 删除文件函数, 删除验证码图片 os.remove(captcha_code) 获取cookie except: pass # 循环执行以下代码 for i in range(3): # 调用 browser 实例中, 执行 JavaScript 代码方法, 以下参数为 JavaScript 代码（执行鼠标下滑操作） browser.execute_script('window.scrollTo(0, document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage;') time.sleep(3) # 获取 cookies cookies = browser.get_cookies() # 列表推导式获取 cookie-dict, 保存对应 cookie, 调用工具类 dict 合并方法, 合并 dict cookie = DataConvert.merge_dicts([{cookie['name']: cookie['value']} for cookie in cookies]) # 存储 cookie pickle.dump(cookie, open(BASE_DIR + '/cookie/weibo.cookie', 'wb')) for url in self.start_urls: yield scrapy.Request(url=url, cookies=cookie, dont_filter=True)","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/Selenium+Chrome%E5%BE%AE%E5%8D%9A%E7%99%BB%E5%BD%95/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"Selenium 简介","date":"2020/08/09","text":"简单实例：from selenium import webdriver from scrapy.selector import Selector browser = webdriver.Chrome(executable_path='E:/Template/chromedriver.exe') browser.get('https://www.zhihu.com/signin?next=%2F') # 若 css 某种写法无法找到元素，则换一种写法 browser.find_element_by_css_selector('.SignFlow-tabs div:nth-child(2)').click() # 找到密码登录按钮，并进行点击 browser.find_element_by_css_selector('.SignFlow-account input[name=\"username\"]').send_keys('17688718015') # 找到账号输入框，并输入密码 browser.find_element_by_css_selector('.SignFlow-password input[name=\"password\"]').send_keys('tanling.') # 找到密码输入框，并输入密码 browser.find_element_by_css_selector('form[class=\"SignFlow Login-content\"] button[type=\"submit\"]').click() # 找人登录按钮，并进行点击 登录失败：​ 因为服务器识别到了 Driver 客户端 ​ 服务器返回：error: {code: 10001, message: “10001:请求参数异常，请升级客户端后重试”} 解决办法： 下载 Chrome 60 版本，Driver 2.33 版本 不推荐 手动启动 ChromeDriver 调试模式 推荐 不是因为 header 中的参数，而是 chromedriver 中有一些 js 变量 变量中包含 chromedriver 的关键词。 1. 启动 chrome 2. CMD 进入 chrome.exe 目录下 3. 指定端口，启动调试端口，输入命令：chrome.exe --remote-debugging-port=9222 4. 访问：127.0.0.1:9222/json 确保chromedriver 实例已关闭完毕 避免追加输入信息# 调用按键代码集类，中CONTROL的值，加上 a 键，进行全选输入栏信息，进行覆盖 # 传入键值元素参数至 send_keys 方法中 browser.find_element_by_css_selector('').send_keys(Keys.CONTROL + 'a') browser.find_element_by_css_selector('').send_keys('17688718015') mouse（鼠标模拟点击库） 避免 selenium 中 click 方法偶尔失效 使用 pip 进行安装 引入 mouse 库中 move，click 方法 传入x，y 轴参数，在调用 click 方法点击 from mouse ifrom mouse import move, clickmport move, click time.sleep(3) move(950, 675) click() 获取 browser 里面的 cookie COOKIES_ENABLED（启用 cookie）从上个 Request 返回数据中 获取 Cookie 自动在下个 Request 中*携带*** COOKIES_ENABLED = True USER_AGENT（用户代理）后续所有请求，都会携带这个 header 参数 USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36' DOWNLOADER_MIDDLEWARES（下载器中间件）DOWNLOADER_MIDDLEWARES = { # 'ArticleSpider.middlewares.ArticlespiderDownloaderMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 2 } Chrome旧版本，WebDriver 启动 Chrome 失败selenium.common.exceptions.WebDriverException: Message: unknown error: cannot find Chrome binarySelenium环境搭建的问题引起最多的是No browser is open，这种报错导致浏览器无法打开；问题的原因主要是2点 1.浏览器安装默认路径 2.浏览器匹配的驱动没有放在正确的环境变量目录位置（可以代码指定驱动路径） 3.浏览器版本与对应的驱动版本匹配 解决办法： from selenium import webdriver options = webdriver.ChromeOptions() # 实例化 ChromeOptions 类 options.binary_location = \"chrome.exe PATH\" # 传入 chrome.exe 文件路径 browser = webdriver.Chrome(chrome_options=options) # 传入实例化对象 -> 关键字参数 chrome_options","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/selenium%20%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E7%AE%80%E4%BB%8B/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"Selenium 基本使用","date":"2020/08/09","text":"Selenium简介​ 自动化测试框架，最开始是用来做Web自动化测试的，测试网站系统开发。 Selenium 是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），[Mozilla Firefox](https://baike.baidu.com/item/Mozilla Firefox/3504923)，Safari，Google Chrome，Opera等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成 .Net、Java、Perl等不同语言的测试脚本。 Selenium 2.0适用于以下浏览器： Google Chrome Internet Explorer 7, 8, 9, 10, 11 Firefox Safari Opera HtmlUnit phantomjs Android iOS 安装pip install selenium安装驱动程序Selenium需要驱动程序才能与所选浏览器交互 Selenium 只是一个 中间 API interface 而已 搜索Python selenium API -&gt; WebDriver -&gt; chromedriver from selenium import webdriver from scrapy.selector import Selector # 实例化 webdriver 包 -> Chrome 模块 -> WebDriver 类，输入 executable_path (可执行路径)参数 browser = webdriver.Chrome(executable_path='E:/Template/chromedriver.exe') # 调用 get 方法，传入url，启动 chromedriver.exe browser.get('https://item.jd.com/68540038032.html') \"\"\" selenium 自带元素选择器 不建议使用，因为 selenium 是纯 Python 编写的。 运行速度是赶不上 scrapy.selector的， 因为selector 是用 lxml 库完成的，lxml是C语言编写的 但是，在某些时候，我们还是需要用到自带的 selector 比如，我们通过 id 获取到某个 button 后，对它进行点击操控 页面元素提取，还是使用 scrapy.selector \"\"\" # browser.find_element_by_css_selector() # 打印运行JS之后的网页源 # print(browser.page_source) # 传入 page_source 至 text 参数，并实例化 t_selector = Selector(text=browser.page_source) # 调用实例化对象中的 css 方法 print(t_selector.css('.p-price span[class=\"price J-p-68540038032\"]::text').extract()) print(t_selector.css('.pricing #page_origin_price::text').extract()) # 关闭 WebDriver browser.quit() 简单实例：from selenium import webdriver from scrapy.selector import Selector browser = webdriver.Chrome(executable_path='E:/Template/chromedriver.exe') browser.get('https://www.zhihu.com/signin?next=%2F') # 若 css 某种写法无法找到元素，则换一种写法 browser.find_element_by_css_selector('.SignFlow-tabs div:nth-child(2)').click() # 找到密码登录按钮，并进行点击 browser.find_element_by_css_selector('.SignFlow-account input[name=\"username\"]').send_keys('17688718015') # 找到账号输入框，并输入密码 browser.find_element_by_css_selector('.SignFlow-password input[name=\"password\"]').send_keys('tanling.') # 找到密码输入框，并输入密码 browser.find_element_by_css_selector('form[class=\"SignFlow Login-content\"] button[type=\"submit\"]').click() # 找人登录按钮，并进行点击 登录失败：​ 因为服务器识别到了 Driver 客户端 ​ 服务器返回：error: {code: 10001, message: “10001:请求参数异常，请升级客户端后重试”} 解决办法： 下载 Chrome 60 版本，Driver 2.33 版本 不推荐 手动启动 ChromeDriver 调试模式 推荐 不是因为 header 中的参数，而是 chromedriver 中有一些 js 变量 变量中包含 chromedriver 的关键词。 1. 启动 chrome 2. CMD 进入 chrome.exe 目录下 3. 指定端口，启动调试端口，输入命令：chrome.exe --remote-debugging-port=9222 4. 访问：127.0.0.1:9222/json 确保chromedriver 实例已关闭完毕 避免追加输入信息# 调用按键代码集类，中CONTROL的值，加上 a 键，进行全选输入栏信息，进行覆盖 # 传入键值元素参数至 send_keys 方法中 browser.find_element_by_css_selector('').send_keys(Keys.CONTROL + 'a') browser.find_element_by_css_selector('').send_keys('17688718015') mouse（鼠标模拟点击库） 避免 selenium 中 click 方法偶尔失效 使用 pip 进行安装 引入 mouse 库中 move，click 方法 传入x，y 轴参数，在调用 click 方法点击 from mouse ifrom mouse import move, clickmport move, click time.sleep(3) move(950, 675) click() 获取 browser 里面的 cookie COOKIES_ENABLED（启用 cookie）从上个 Request 返回数据中 获取 Cookie 自动在下个 Request 中*携带*** COOKIES_ENABLED = True USER_AGENT（用户代理）后续所有请求，都会携带这个 header 参数 USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36' DOWNLOADER_MIDDLEWARES（下载器中间件）DOWNLOADER_MIDDLEWARES = { # 'ArticleSpider.middlewares.ArticlespiderDownloaderMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 2 } Chrome旧版本，WebDriver 启动 Chrome 失败selenium.common.exceptions.WebDriverException: Message: unknown error: cannot find Chrome binarySelenium环境搭建的问题引起最多的是No browser is open，这种报错导致浏览器无法打开；问题的原因主要是2点 1.浏览器安装默认路径 2.浏览器匹配的驱动没有放在正确的环境变量目录位置（可以代码指定驱动路径） 3.浏览器版本与对应的驱动版本匹配 解决办法： from selenium import webdriver options = webdriver.ChromeOptions() # 实例化 ChromeOptions 类 options.binary_location = \"chrome.exe PATH\" # 传入 chrome.exe 文件路径 browser = webdriver.Chrome(chrome_options=options) # 传入实例化对象 -> 关键字参数 chrome_options","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/selenium%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"Scrapy 集成 Selenium","date":"2020/08/09","text":"设置中间件集成 selenium driver，实现动态加载以下方法改变了scrapy异步加载，变为了同步加载，会降低运行效率，当然也可以解决，需要重写 downloader（至少要了解 twisted API) 定义中间件class JSPageMiddleware(object): \"\"\"通过 chrome 请求动态网页\"\"\" def process_request(self, request, spider): \"\"\"selenium 模拟访问\"\"\" # 判断 spider 实例的 name if spider.name == 'jobbole': from scrapy.http import HtmlResponse import time # 请求 URL spider.browser.get(request.url) # 放大窗口 / 严谨一点可使用 try spider.browser.maximize_window() # 避免加载过慢 time.sleep(3) print('request:{0}'.format(request.url)) # 避免重复下载, 直接调用 HtmlResponse 进行返回 # current(当前) browser 请求 url, page_source(页面源) 的 html 页面 # 因为 HtmlResponse 的父类 TextResponse 中 类变量 _DEFAULT_ENCODING 默认为 ascii 码, 根据源页面编码指定编码 return HtmlResponse(url=spider.browser.current_url, body=spider.browser.page_source, encoding='utf-8', request=request) 使用 Spider 构造函数，创建 browser 实例def __init__(self): chrome_option = webdriver.ChromeOptions() chrome_option.add_argument('blink-settings=imagesEnabled=false') # chrome_option.add_argument('--disable-extensions') # chrome_option.add_experimental_option('debuggerAddress', '127.0.0.1:9222') self.browser = webdriver.Chrome(executable_path='E:/Template/chromedriver.exe', chrome_options=chrome_option) # 使用 super 方法, 防止重复调用 super(JobboleSpider, self).__init__() 信号 Scrapy广泛使用信号来通知某些事件的发生。您可以捕获Scrapy项目中的某些信号（例如，使用扩展名）来执行其他任务，或者扩展Scrapy以添加开箱即用的功能。 即使信号提供了几个参数，捕获它们的处理程序也不需要接受所有参数-信号分配机制将只传递处理程序接收的参数。 您可以通过Signals API连接到信号（或发送自己的 信号）。 优点：可以允许自定义很多逻辑，且不会侵入到Scrapy的代码当中，跟 Django的设计框架是很相像的 使用信号控制，关闭 browser 实例from scrapy.signalmanager import dispatcher from scrapy import signals def __init__(self): chrome_option = webdriver.ChromeOptions() chrome_option.add_argument(&#39;blink-settings=imagesEnabled=false&#39;) # chrome_option.add_argument(&#39;--disable-extensions&#39;) # chrome_option.add_experimental_option(&#39;debuggerAddress&#39;, &#39;127.0.0.1:9222&#39;) self.browser = webdriver.Chrome(executable_path=&#39;E:/Template/chromedriver.exe&#39;, chrome_options=chrome_option) # 使用 super 方法, 防止重复调用 super(JobboleSpider, self).__init__() # 调用 dispatcher(分发/调度器) 模块中的 connect 函数 # 第一个参数 receiver, 接收一个处理方法的名称 # 第二个参数 signal, 接收一个信号, 传入 signals(信号池) 中的某个信号 # spider_closed: 当 Spider close 的时候, 给出的信号 # 分发器调用传入方法 dispatcher.connect(self.spider_closed, signals.spider_closed) def spider_closed(self, spider): &quot;&quot;&quot;当爬虫关闭的时候, 关闭 browser 实例&quot;&quot;&quot; print(&#39;{} closed execute browser quit&#39;.format(spider.name)) self.browser.quit()","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/selenium%E9%9B%86%E6%88%90%E5%88%B0scrapy/","photos":[]},{"tags":[],"title":"session 与 cookie 的区别","date":"2020/08/09","text":"Cookie 浏览器本地存储方式，存储键值对的格式 简介为什么有cookie的存在，http协议是一种无状态的协议，服务器接收到浏览器的请求后，服务器直接返回内容给浏览器。不管是谁发起的请求 ​ Cookie存储本地，存在隐患。随之引申出 session 根据客户端请求，进行校验。通过后 服务器生成对应的随机加密 id，返回给客户端，且该id是存在有效时间的","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"存储知乎数据","date":"2020/08/09","text":"Scrapy -&gt; Shell 中配置 User-Agent ​ scrapy shell -s USER_AGENT=\"Browser AGENT\" URL 分析URL 源URLhttps://www.zhihu.com/api/v4/answers/640764591/rewarders?include=data%5B*%5D.answer_count%2Carticles_count%2Cfollower_count%2Cis_blocking%2Cis_blocked%2Cis_following%2Cis_followed&amp;offset=0&amp;limit=10 offset = 0 偏移量 = 从哪条开始 limit = 10 限制 = 加载多少条 数据 分析网站URL请求的结构，是非常重要的。它会让我们清楚应该发送什么样的请求 设计数据表结构 question answer 重构 pipeline假如一个 item 对应一个 pipeline，后续上百个或者更多的 item，岂不是需要发起上百次 db 连接，现在需要做的就是一个 pipeline / cursor，处理所有的 item。实际上在真正的开发中，有可能不同的 spider 或一个 spider 爬取的数据，我们需要存储到不同的 db 当中，这个时候可以根据不同的 db 建立 pipeline。 实现一个 pipeline 处理多个 item 在每个 item 中，定义 insert sql 语句，与需要 insert 的 tuple field，作为实例方法，返回出去 def get_insert_sql(self): \"\"\" 定义 insert_sql 语句，以及参数 \"\"\" insert_sql = ''' insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ''' params = ( self.get('zhihu_id', 0), self.get('topics', ''), self.get('url', ''), self.get('title', ''), self.get('content', ''), self.get('answer_num', 0), self.get('comments_num', 0), self.get('watch_user_num', 0), self.get('click_num', 0), self.get('crawl_time', '1970-01-01') ) return insert_sql, params 重构 Twisted 类 -&gt; do_insert 方法 def do_insert(self, cursor, item): \"\"\" cursor（游标）参数，是 Twisted.adbapi 模块，自动注入的 入库逻辑代码 根据不同的 item ， 构建不同的 sql 语句，并插入到 db 中 \"\"\" # 不推荐的用法, 通用性不强 # if item.__class__.__name__ == 'JobBoleArticleItem': # 调用实例化对象 item 中的方法，获取 insert sql 语句，与 tuple field insert_sql, params = item.get_insert_sql() # 传入 sql 语句，以及对应的参数 cursor.execute(insert_sql, tuple(params)) return item","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/%E5%AD%98%E5%82%A8%E7%9F%A5%E4%B9%8E%E6%95%B0%E6%8D%AE/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"Scrapy 暂停与重启","date":"2020/08/09","text":"实现方法如下方法一：进入 Terminal 输入以下命令scrapy crawl spider -s JOBDIR=job_info/001 方法二：配置 settings 组件JOBDIR='job_info/001' 方法三：在 Spider 中自定义配置custom_settings = { 'JOBDIR': 'job_info/001' }","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/Scrapy%E6%9A%82%E5%81%9C%E4%B8%8E%E9%87%8D%E5%90%AF%E7%9A%84%E4%BD%BF%E7%94%A8/","photos":[]},{"tags":[],"title":"PhantomJS 简介","date":"2020/08/09","text":"简述 PhantomJS 是一个基于 webkit 的 JavaScript API。它使用 QtWebKit 作为它核心浏览器的功能，使用 webkit 来编译解释执行 JavaScript 代码。任何你可以在基于 webkit 浏览器做的事情，它都能做到。它不仅是个隐形的浏览器，提供了诸如 CSS 选择器、支持 Web 标准、DOM 操作、JSON、HTML5、Canvas、SVG等，同时也提供了处理文件 I/O 的操作，从而使你可以向操作系统读写文件等。PhantomJS 的用处可谓非常广泛，诸如网络监测、网页截屏、无需浏览器的 Web 测试、页面访问自动化等. 缺点：多进程情况下，PhantomJS 性能会下降很严重 优点：可以在 Linux 各种发行版下运行","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/PhantomJS%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","photos":[]},{"tags":[],"title":"Requests 模拟知乎登录","date":"2020/08/09","text":"常见 http_status_code 200 请求被成功处理 301/302 永久重定向/临时重定向 403 没有权限访问 404 没有相对应的资源 500 服务器错误 503 服务器停机/正在维护 实际上这些 Status_code 一般是由服务器指定的，某些情况下是框架，或自动定义的 代码示例：# -*- coding: utf-8 -*- __author__ = 'Well' import requests try: # py2 中是cookielib import cookielib except: import http.cookiejar as cookielib import re # 实例化 session session = requests.sessions() # 实例化 LWPCookieJar ，以保存 cookies session.cookies = cookielib.LWPCookieJar() try: session.cookies.load(ignore_discard=True) except: print('cookie 未能加载') user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:76.0) Gecko/20100101 Firefox/76.0' header = { 'Host': 'www.zhihu.com', 'Referer': 'https://www.zhihu.com/signin?next=%2F', 'User-Agent': user_agent } def is_login(): # 判断登录状态 rep = session.get('https://www.zhihu.com/settings/account', headers=header, allow_redirects=False) if rep.status_code != 200: return False else: return True def get_xxrf(): # 获取xsrf code response = session.get('www.zhihu.com', headers=header).text return re.match('', response).group(1) def get_index(): # 保存HTML response = session.get('www.zhihu.com', headers=header).text with open('index_page.html', 'wb') as fp: fp.write(response.encode('utf-8')) print('ok') def zhihu_login(account, password): # 知乎登录 if re.match(r'^1\\d{10}', account): post_url = 'https://www.zhihu.com/login/phone' post_data = { 'xsrf': get_xxrf(), 'phone_num': account, 'password': password } elif '@' in account: post_url = 'https://www.zhihu.com/login/email' post_data = { 'xsrf': get_xxrf(), 'phone_num': account, 'password': password } else: print('账号有误') rep = session.post(url=post_url, data=post_data, headers=header).text session.cookies.save()","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/Requests%E6%A8%A1%E6%8B%9F%E7%9F%A5%E4%B9%8E%E7%99%BB%E5%BD%95/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"ChromeDriver 图片加载控制","date":"2020/08/09","text":"设置 chromedriver 不加载图片方法一：# 关闭图片加载, 使得代码运行速度更快 chrome_option.add_argument('blink-settings=imagesEnabled=false') 注意: 该选项在远程调试时不会生效（需要手动进入设置-&gt;隐私和安全性中设置） 方法二：#设置chromedriver不加载图片 prefs = {\"profile.managed_default_content_settings.images\": 2} chrome_opt.add_experimental_option(\"prefs\", prefs)","permalink":"http://yoursite.com/2020/08/09/Python/crawler/senior/chromedriver-%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E6%8E%A7%E5%88%B6/","photos":[]},{"tags":[{"name":"Basics","slug":"Basics","permalink":"http://yoursite.com/tags/Basics/"}],"title":"Python正则表达式","date":"2020/07/12","text":"正则表达式 爬虫的基础掌握 是一个特殊的字符序列 检测一个字符串是否与我们设定的字符序列，相匹配 若匹配，则可以快速检索，替换文本的操作 场景示例1：检测一串数字是否是电话号码 场景示例2：检测一串字符串是否符合 Email 把一个文本中指定单词，替换成另一个单词 爬虫爬取html网页 找到标签，爬取标签中间的内容 正则表达式，主要应用在爬虫，应用数据处理与分析上 常用正则表达式，可使用已经完善的，可搜索到的直接使用 容易遗忘正则表达式，经常使用正则表达式 Python 内置函数 index语法示例：A = 'C|C++|C#|Python|Java|Javascript' print(A.index('Python') > -1) True /表示 常量A中包含str Python 使用成员运算符： print('Python' in A) True /表示 常量A中包含str Python 引入Python内置模块 re re 中有很多方法可操作正则表达式 语法示例：import re #导入内置模块 A = 'C|C++|C#|Python|Java|Javascript' #把字符串赋值给常量A B = re.findall('Python', A) #常量B,接收表达式结果 print(B) #返回 ['Python'] 实例：import re #导入内置模块 A = 'C|C++|C#|Python|Java|Javascript' #把字符串赋值给常量A B = re.findall('PHP', A) #常量B,接收表达式结果 if len(B) > 0: print('字符串中包含Python') else: print('字符串中无查询选项') 正则表达式在于 规则 ，而不是表达普通的常量字符串 组成元素 普通字符re.findall('Python', 常量) 'Python'中的字符，为普通字符 元字符 （字符集，概括字符集，数量词)re.findall('\\d', 常量) '\\d'中的字符，为元字符 正则表达式，就是由一系列普通字符，元字符所组成的 且普通字符和元字符，可混合使用 字符集# 字符集 import re s = ‘abc, acc, adc, aec, afc, ahc’ z = re.findall(‘a[^cf]c’, s) a 和 c 作为普通字符 用作定界 [cf]作为元字符进行抽象检索 匹配c or f 或关系，[^cf] 非c，非f 取反的操作[a-f] 按顺序匹配，彼此为或关系，缩短代码长度 print(z) ``` 概括字符集 不管是字符集/概括字符集，都是匹配一个字符 Command Describe [] 中括号 括号中的字符为或关系 \\d 匹配0-9阿拉伯数字 \\D 匹配非数字 \\w 匹配单词字符(数字以及字母，下划线) [A-Z a-z 0-9 _] \\W 匹配非单词字符 (&amp;,*,\\n, \\r) \\s 匹配空白字 符(\\n , \\r, \\t, ) \\S 匹配非空白字符 . 匹配除换行符外的所有字符 | 或关系 “1|2” 一或二 [.*] 中括号中. * 即为普通字符元素 [^\\d] 中括号中 为非关系 \\u4E00-\\u9FA5 匹配中文 数量词贪婪与非贪婪使用不当 ，易导致程序bug 贪婪#数量词 import re a = 'Python 1111Java678php' b = re.findall('[A-Za-z]{3,6}', a) # 匹配大小写字母，单词数量区间为3 - 6个字符 # 为什么匹配时，没在 'Pyt' 处截断 , 一直匹配至 Python # 默认匹配方式为，贪婪 优先取设定区间最大的数量 当匹配至，不在符和条件时停下 ->跳过 ->寻找下个目标字符 print(b) # ['Python', 'Java', 'php'] 非贪婪 加入字符 ?import re a = 'Python 1111Java678php' b = re.findall('[A-Za-z]{3,6}?', a) # 匹配大小写字母，单词数量区间为3 - 6个字符 非贪婪 # 优先取设定区间最小数量，当匹配至，不在符和条件时停下 ->跳过 ->寻找下个目标字符 print(b) # ['Pyt', 'hon', 'Jav', 'php'] 匹配 0次 或 无限多次（去重等功能）import re a = 'pytho0python1pythonn2' b = re.findall('python*', a) # 匹配0次或无限多次 pytho中的 n print(b) # ['pytho', 'python', 'pythonn'] 匹配1次 或 无限多次import re a = 'pytho0python1pythonn2' b = re.findall('python+', a) # 匹配1次或无限多次 pytho中的 + print(b) # ['python', 'pythonn'] ? 匹配0次 或 1次import re a = 'pytho0python1pythonn2' b = re.findall('python?', a) # 匹配0次或 1次 pytho中的 ? print(b) # ['pytho', 'python', 'python'] 边界匹配 ^ $ # 边界匹配 # ^ 从字符串前面 开始匹配 仅匹配一次 结果为：有 或 空 # $ 从字符串后面 开始匹配 仅匹配一次 结果为：有 或 空 看为一个占位符 # ^ $ 匹配完整字符串 位数 import re qq = &#39;000000001&#39; # 需求：账号为4~8位之间 b = re.findall(&#39;^\\d{4,9}$&#39;, qq) # 正则匹配：贪婪 print(b) # [&#39;000000001&#39;] 从前到后，整组字符串 进行 区间 贪婪匹配 c = re.findall(&#39;00{2,3}$&#39;, qq) print(c) # [] d = re.findall(&#39;^00{2,3}&#39;, qq) print(d) # [&#39;0000&#39;] 仅匹配一次 0 and 匹配条件 0 {2,3}区间，默认贪婪 0 +000 e = re.findall(&#39;^100$&#39;, qq) print(e) # [] ``` 组 () 圆括号中的字符为 且关系# 组 使用()中的字符 import re a = 'PythonPythonPythonPythonPython' b = re.findall('(Python){6}', a) print(b) 模式参数 ，findall函数的第三个参数 re.I 无视字符大小写#模式参数 import re lanuage = 'PythonC#PHPJava' b = re.findall('c#', lanuage, re.I) print(b) # ['C#'] 可接受多个模式，之间需要 | 连接在一起 re.S匹配所有字符包括换行符#模式参数 import re lanuage = 'PythonC#\\nPHPJava' b = re.findall('c#.{1}', lanuage, re.I | re.S) #匹配c# + 一个任意字符 除\\n换行符 re.I 与 re.S 为且关系 print(b) # ['C#'] 正则替换 使用re模块中的re.sub#正则替换 import re lanuage = 'PythonC#JavaPHPPython，Python' b = re.sub('(Python)', 'python', lanuage, 2) # 把组中字符，替换为python，紧接着为字符串，count参数 默认为0 = 无限替换 print(b) 使用Python内置函数 replace#正则替换 import re lanuage = 'PythonC#JavaPHPPython，Python' lanuage = lanuage.replace('P', 'p') # 内置函数 replace 替换字符串 print(b) 传入函数进行替换#正则替换 import re def convert(value): return '22' lanuage = 'PythonC#JavaPHPPython，Python' b = re.sub('Python', convert, lanuage, 2) # 正则匹配，若有结果，则传入convert函数中，函数返回的值将进行替换 Print(b) 调用函数传递参数 #调用 （目的是把业务交给函数去处理） def convert(value): print((value)) convert 函数被调用两次 每次传入的value 为一个对象 &lt;re.Match object; span=(0, 6), match=&#39;Python&#39;&gt; 序号 = 前面0位 至 截止六位 &lt;re.Match object; span=(15, 21), match=&#39;Python&#39;&gt; # return &#39;2&#39; + value + &#39;2&#39; lanuage = &#39;PythonC#JavaPHPPython，Python&#39; b = re.sub(&#39;Python&#39;, convert, lanuage, 2) ``` 接收对象，并进行判断操作（重要） def convert(value): # 接收 使用group方法转换的字符串 matched = value.group() a = 'Python' # 判断字符串，进行条件转换 if matched == 'Python': matched = 'C++' elif matched == 'C#': matched = 'TypeScript' else: pass return '已处理' + matched lanuage = 'PythonC#JavaPHPPython，Python' b = re.sub('Py', convert, lanuage, 2) Python函数 / 方法中可接受 函数 （不只是str，float，int等类型） re模块中的函数 Findall 传入正则表达式，字符串，模式参数 返回list类型中加入字符串 Sub 传入正则表达式，替换参数，原字符串，替换次数，模式参数 Match 传入正则表达式，字符串，模式参数 从字符串的第一位开始匹配，若没有则返回None 仅匹配一次 Search 传入正则表达式，字符串，模式参数 从字符串前面往后一直匹配，成功后，立马返回结果 仅匹配一次 Group 转换object类型，为字符串 分组 可传入参数 = 组号 示例代码import re s = 'life is short,i use Python' r = re.search(' (.*) ', s) # 上述正则表达式未一个分组， 虽然未加入()，因为只有一个分组 print(r.group(1)) # 传入组号，默认为0 记录的是：正则表达式的完整匹配结果 # 想要访问完整匹配结果，下面的分组 需填入>=1 可连续传入多个组号 print(r.group(1, 2, 3)) (&#39;life is short,i use Python, i love Python&#39;, &#39; is short,i use &#39;, &#39;, i love &#39;) 查询结果以tuple类型返回 ``` Groups 不需要传入组号参数print(r.groups()) 返回已定义分组，不会饭后整个字符串 Span返回object类型的位置","permalink":"http://yoursite.com/2020/07/12/Python/basic/re/","photos":[]},{"tags":[{"name":"Basics","slug":"Basics","permalink":"http://yoursite.com/tags/Basics/"}],"title":"JSON与XML区别","date":"2020/07/12","text":"JSON简述：JavaScript Object Notation 译为 JavaScript 对象标记 本质概述：是一种轻量级的数据交换格式 再次强调 JSON是 一种数据交换格式 字符串是JSON的表现形式/载体 JSON对象 与 JSON字符串的区别 符合JSON格式的字符串叫做JSON字符串 错误JSON格式{a:\"Python\" 正确JSON格式{a:\"Python\"} JSON和每种语言下特定的数据结构，进行数据交换，转换 JSON对象， JSON， JSON字符串 之间的区别 绝大多数的答案，都站在JavaScript语言的角度，阐述区别 JSON是REST服务的标准格式 JSON的制定并不是专门为JavaScript设计的 它只是一种传输数据的一种格式 JavaScript与JSON的数据交换里面，和Python，C#没有区别 JSON的数据类型与JavaScript过于相似 JSON有自己的数据类型，仅和JavaScript相似 JavaScript只是一个实现标准方案之一 标准为：ECMASCRIPT -&gt; ActionScript -&gt; JSON的一个版本 某种程度JSON和JavaScript是一种平级的语言 误区（JSON与JavaScript，没有直接关系）JSON与JavaScript都是对ECMASCRIPT这个W3C（团队）它所制定的脚本的规范和标准实现 JSON在最开始在前后端的分离起到了重要的作用，而服务器的语言种类繁多，前端最主流的就是JavaScript，因为JSON被大量的应用在JavaScript的交互中 还有一个微软的TypeScript只是JavaScript的超集 JSON对象(1) 定义非常片面（在JavaScript中是存在的） (2) 放到 Python中的话，是没有这个JSON对象的 轻量级的数据格式 外部数据交换的主流格式 XML 使用较少，特殊情况例外 在JSON出现之前，为主流交换格式 在特定的领域，注重数据结构的领域会使用 阅读稍显复杂 跨语言交换数据 JSON的优势 易于阅读 易于解析 JSON作为数据交互的格式，存在组装和被解析的过程 网络传输效率高 数据量同XML相比，要更少 跨语言交换数据 Python调用C++，Java调用.Net 把某一种语言的功能，做成一个服务，利用这个服务的特性使用JSON来进行传递数据 web结构 反序列化 当你拿到的JSON，不是一个str，而是一个object，这是框架默认把str转换成一个object，但是我们原生拿到的就是一个str 拿到JSON，能够快速访问JSON字符串中相关的信息 把JSON字符串转换成Python中已知的数据结构 反序列化过程# 引入内置json模块，其中包含一系列操纵JSON数据的方法 import json # json格式中，字符串的key 必需使用双引号，bool值为小写 json_str = '{\"name\":\"qiyue\", \"age\":18, \"2\":false}' JavaScript中为object数据结构 # 调用json模块中的内置方法，转换json字符串类型，转换为Python数据类型 Student = json.loads(json_str) print(Student) print(type(Student)) 转换后部分JSON格式特征，会转换为Python特征 {'name': 'qiyue', 'age': 18, '2': False} Python用dict类型，承载JSON中的object类型 &lt;class 'dict'> 同一JSON字符串，在不同语言中，可能转换成不同的数据类型 这个转换过程 称为 *反序列化*** 拓展：JSON数据类型 array（数组）# 反序列化 # 引入内置json模块 import json as x json_str = '[{\"name\":\"qiyue\", \"age\":18, \"2\":false}, {\"name\":\"qiyue\", \"age\":18, \"2\":false}]' # 包含两个对象的数组 Student = x.loads(json_str) # 调用json模块中的内置方法，转换json格式字符串 print(Student) print(type(Student)) Python用list类型，承载JSON中的array类型 print(Student[1]['name']) 访问数组中对象的key 数组表示为一种集合，类似与 Python 中的 list，tuple 。只不过Python把集合分的比较细 比如 PHP 中只有一个数组，且非常强大的 这个是不同语言对集合不同的解构形式 数组其实就是一个集合 JSON 对应 Python 的数据类型 JSON Python object dict array list string str number int number float true/false True/False null None 若精通一门语言，入门其他语言相对比较容易 因为绝大数语言，对于基本数据结构的定义都大同小异 例如循环语句，面向对象等 现在很多语言就是经典C，C语言的语法糖，内部的实现机制，内存管理不同 现在很多语言就是类 C语言 ，语言就是一种工具 序列化把Python的数据类型，向JSON字符串转换的过程 序列化过程import json Student = [ {'name': 'qiyue', 'age': 18, '2': False}, {'name': 'qiyue', 'age': 18} ] json_str = json.dumps(Student) # dumps 接收一个object参数，dict 也是属于 object print(json_str) # [{\"name\": \"qiyue\", \"age\": 18, \"2\": false}, {\"name\": \"qiyue\", \"age\": 18}] print(type(json_str)) # &lt;class 'str'> 不仅是 Python 数据向 JSON 转换，是序列化 把 XML 的字符串向 Python 转换，也是反序列化 / 序列化 怎么把一个object存储到数据库里面去？ 数据库是一个个的二维表，没有办法去表示object 错误示例：把 object 序列化成 JSON 字符串，或 XML 字符串 然后把字符串，存储到数据库里面去 需要的时候，从数据库里面读取出来，然后进行反序列化 *效率低 / 不可取*** 正确示例：MySQL 等数据库适合存储比较简单的数据结构 应该把 object 拆分成一个个属性，进行存储 PS：实在是想存储 object，可使用 NOSQL MongoDB 序列化的意义一般通过服务获取到其他语言进行序列化的 JSON 字符串 XML 数据格式代码&lt;?xml version=\"1.0\" encoding=\"UTF-8”?> &lt;note> &lt;to>Tove&lt;/to> &lt;from>Jani&lt;/from> &lt;heading>Reminder&lt;/heading> &lt;body>Don’t forget me this weekend! &lt;/body> &lt;/note>","permalink":"http://yoursite.com/2020/07/12/Python/basic/json-xml/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"Python扩展知识","date":"2020/07/12","text":"导语产品是需要打磨出来的，重点在于打磨二字，没有什么产品可以一就而成 字典替代switch Switch语句同样为条件分支语句 其他语言中Switch语句的作用，C# 字典映射替代switch语句 示例：def get_Sunday(): return 'Sunday' def get_Monday(): return 'Mnoday' def get_Tuesday(): return 'Tuesday' # 返回字符串常量 def get_default(): return 'Unkown' # 使用字典代替switch语句 day = 2 switcher = { 0 : get_Sunday, # key对应的value，不仅仅是字符串，还可以是function类型 1 : 'Monday', 2 : 'Tuesday' } # 比switch语句，简洁，可读性强 # 使用get函数，若dict中包含输入参数key，则返回对应的value，否则返回默认值 day_name = switcher.get(day, get_default) # ()代表执行函数 if hasattr(day_name, '__call__'): # 判断对象是否为function， 建议默认值，也使用方法 print(day_name()) else: print(day_name) # print(day_name) # print(type(day_name)) 列表推导式 功能相同方法for in循环 map函数 根据已有的列表，创建新的列表 类似数学中的 《集合推导式》 简洁代码 不仅限于对list进行推导，也可以包含set tuple str dict数据结构 语法示例 a = [1, 2, 3, 4, 5, 6, 7, 8] # 根据变量a，创建一个新的变量b，在a列表中的每个元素，进行平方运算 具有Python风格特色的列表推导式 b = [i**2 for i in a] print(b) - ###### 场景示例，加入条件判断 ```python a = [1, 2, 3, 4, 5, 6, 7, 8] # 根据变量a，创建一个新的变量b，在a列表中的每个元素，进行平方运算 # 场景示例，选择性改变元素 # 加入条件判断进入，列表推导式中 b = [i**3 for i in a if i &gt;= 5] print(b) str推导式# str推导式 x = '1, 2, 3, 4, 5, 6, 7, 8' # 使用字符串中的有序元素，赋予对应的value，组成dict xs = {y:1 for y in x} print(xs) Tuple推导式# tuple推导式 Apple = (1, 2, 3, 4, 5) Orange = (x for x in Apple if x == 3) print(Orange) # 打印出对象，因为tuple为不可变 for orange in Orange: print(orange) Dict推导式# dict推导式 students = { '石小乐': 18, '王小明': 16, '李飞飞': 17 } # 实际使用两个参数，去解包dict，可进行颠倒key，value，或取出其中某个值 # 解包需要调用内置items函数，返回的值，在tuple中不能直接使用 student = {value: key for key, value in students.items()} print(student)","permalink":"http://yoursite.com/2020/07/12/Python/basic/extend/","photos":[]},{"tags":[{"name":"UDP","slug":"UDP","permalink":"http://yoursite.com/tags/UDP/"},{"name":"Socket","slug":"Socket","permalink":"http://yoursite.com/tags/Socket/"}],"title":"套接字","date":"2020/07/12","text":"Socket简介不同电脑上的进程之间如何通信 首先解决的问题是如何唯一标识一个进程，否则通信无从谈起。 在一台电脑上可以通过进程号(PID) 来唯一标识一个进程，但是在网络上这是行不通的。 其实TCP/IP协议族已经帮我们解决了这个问题，网络层的“ip地址”可以标识唯一网络中的主机，而传输层的“协议+端口”可以唯一标识主机中的应用程序(进程)。 这样利用ip地址 + 协议 + 端口 就可以标识网络的进程了，网络中的进程通信就可以利用这个标志与其他进程进行交互 注意： “进程”：运行的程序以及运行时用到的资源这个整体称之为进程 “进程间通信”：运行的程序之间的数据共享 什么是Socket Socket(简称：套接字)是进程间通信的一种方式，它与其他进程间通信的一个主要不同是： 它能实现不同主机间的进程通信，我们网络上各种各样的服务大多同时基于Socket来完成通信的，例如我们每天浏览网页，QQ聊天，收发Email等等。 创建Socket 在Python中使用socket模块中的socket方法就可以完成 socket.socket(Family, Type) 简述: socket类创建的对象，该对象带有两个默认参数: family: 可以选择AF_INET(用于internet进程间通信) 或者AF_UNIX(用于 同一台机器进程间通信)， 实际工作中常用AF_INET type: 套接字类型，可以是SOCKET_ STREAM(流式套接字， 主要用于TCP协议)或者SOCK_ DGRAM(数据报套接字，主要用于UDP协议) 发送UDP数据 import socket def main(): # 创建一个udp的套接字 udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # 循环发送，并支持exit指令退出程序 while True: ##### 接收键盘输入信息 content = input('请输入内容： ') if content == 'exit': break # udp_socket.sendto(数据, 目的IP与Port) udp_socket.sendto(content.encode('utf-8'), ('10.0.0.1', 8080)) # 关闭套接字 udp_socket.close() if __name__ == '__main__': main() 接受UDP数据如果程序要接收数据，必须固定端口 代码如下：from socket import * def main(): # 1. 创建udp套接字 udp_socket = socket(AF_INET, SOCK_DGRAM) # 2. 添加本地信息 local_address = ('', 7878) 必须是本机的ip port udp_socket.bind(local_address) # 3. 接收数据 receive_data = udp_socket.recvfrom(1024) 数据bytes大小 接收的数据是一个tuple类型，包含发送方的数据，ip，port(b’data’, (‘192.168.0.0’ , port)) receive_msg = receive_data[0].decode('gbk') # 存储接收的信息 send_address = receive_data[1] # 存储发送方的地址 # 4. 打印数据 print('%s:%s' % (str(send_address), receive_msg)) # 5. 关闭套接字 udp_socket.close() if __name__ == '__main__': main() 绑定端口 当网络程序开始运行，进行收发数据，未进行固定端口，系统则随机分配一个端口，结束进程则释放该端口。 一般接收方需事先绑定端口 注意：UDP中端口不能被占用常见问题recvfrom 在数据没有到来是会怎样？ 调用recvfrom之后，数据未到 它会处于堵塞状态 没有调用recvfrom之前，数据已到 数据首先进入操作系统缓存区，再调用recvfrom找到数据 注意：易导致操作系统内存占满 socket是 全双工？​ 套接字是一个，可以同时 收发数据的 飞秋通信用的是2425端口，那么qq呢？qq怎样通信？扩展知识 单工：收音机此类，无法发送数据，只能接收数据 半双工：对讲机，可以收发数据，但不能同时收发 全双工：可以同时收发数据","permalink":"http://yoursite.com/2020/07/12/Python/network/UDP/socket/","photos":[]},{"tags":[{"name":"Port","slug":"Port","permalink":"http://yoursite.com/tags/Port/"}],"title":"端口","date":"2020/07/12","text":"什么是端口 网络通信必备IP ，端口 端口就好比是一个房子的门，是出入这间房子的必经之路。 扩展知识：一个程序没有运行之间称之为“程序”，程序运行起来之后称为“进程” 如果一个程序需要收发网络数据，那么就需要有这样的端口 在linux系统中，端口可以有65536（2的16次方）个之多 既然有这么多，操作系统为了同一管理，所以进行了编号，这就是端口号 端口号 端口是通过端口号来标记的，端口号只有整数，范围是从0到65535 注意：端口数不一样的nix系统不一样，还可以手动修改。 端口是怎样分配的 端口号不是随意使用的，而是按照一定的规定进行分配的。 端口的分类标准有好几种，我们这里不做详细讲解，只介绍一下知名端口和动态端口 知名端口（Well Known Ports） 知名端口就是众所周知的端口号，范围从0到1023 80端口分配给HTTP服务 21端口分配给FTP服务 一般情况下，如果一个程序需要使用知名端口，需要获得ROOT权限 动态端口（Dynamic Ports） 动态端口的范围是从1024到65535 之所以称为动态端口，是因为这个范围的端口号未绑定某个服务，而是动态分配。 动态分配是指当一个系统程序或应用程序需要网络通信时，它向主机申请一个端口，主机从可用的端口号中分配一个供它使用。 当这个程序关闭时，同时也就释放了它所占用的端口号 怎样查看端口号 命令：netstat -an 查看端口状态 命令：lsof -i [tcp/udp]:2425 小总结 端口有什么用呢？ ​ 我们知道，一台拥有IP地址的主机可以提供许多服务，比如HTTP（万维网服务），FTP（文件传输），SMTP（电子邮件）等，这些服务完全可以通过一个IP地址来实现。 那么，主机怎样区分不同的网络服务呢? 显然不能只靠IP地址，因为IP地址与网络服务的关系是一对多的关系。 实际上是通过IP地址 + 端口号来区分不同的服务的。 需要注意的是，端口并不是一一对应的。 比如你的电脑作为客户机访问一台WWW服务器时，WWW服务器使用“80”端口与你的电脑通信，但你的电脑则可能使用“1234”这样的端口。","permalink":"http://yoursite.com/2020/07/12/Python/network/UDP/port/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"网络基础","date":"2020/07/12","text":"什么是网络 通信方式进行传输数据，这就是网络的一种体现 对讲机也是一种无线电电磁波传输信息，这就是网络。 网络不仅仅是可视化网线 简述： 网络就是一种辅助双方或者多方能够连接在一起的工具 如果没有网络可想单机世界是多么的孤独 使用网络的目的 为了联通多方然后进行通信的，即把数据从一方传递给另一方 为了让在不同的电脑上运行的软件，之间能够互相传递数据，就需要网络 小总结： 使用网络能够把多方连接在一起，然后可以进行数据传递 所谓的网络编程就是，让在不同的电脑上运行的程序进行数据传递，即进程之间的通信 同一公司，班级传输数据是一个局域网 不同城市，国家专属数据是万维网，互联网 不管是蓝牙，WIFI，网线都是网络功能","permalink":"http://yoursite.com/2020/07/12/Python/network/UDP/network-basic/","photos":[]},{"tags":[{"name":"Basics","slug":"Basics","permalink":"http://yoursite.com/tags/Basics/"}],"title":"Linux命令","date":"2020/07/12","text":"查看操作系统的网卡信息Ubuntu系统 安装查看工具：sudo apt-get install net-tools 安装完毕后输入：ifconfig 以太网：ens_xxx 本地网卡环回：io 关闭以太网网卡：sudo ifconfig ensxx down 开启以太网网卡：sudo ifconfig ensxx up Windows系统 查看命令：ipconfig","permalink":"http://yoursite.com/2020/07/12/Python/network/UDP/linux-cmd/","photos":[]},{"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"IP地址","date":"2020/07/12","text":"IP地址什么是IP地址 用来标记网络上的一台电脑 局域网ip地址可以相同，但会导致冲突（数据丢失） 所以ip地址允许重复 IP地址的作用 IP地址：用来在网络中标记一个终端，比如192.168.1.1; 在本地局域网内是唯一的。 IP地址的分类（了解） 每一个IP地址包括两部分：网络地址与主机地址 pv4 全称 internet protocol version 4 概述：Ip地址标记网络里面的第四种版本 格式：xxx.xxx.xxx.xxx 最大值是：255 最小值是：0 总数应该是：256256256*256 拓展：ip v1-v3-v5是实验版本 Ipv6 全称 internet protocol version 6 ​ 概述：因为ipv4的IP段使用已饱和，所以诞生ipv6，且尚未普及 ​ 格式：x:x:x:x:x:x:x:x，其中每个 x 是地址的 8 个 16 位部分的十六进制值。 ​ IPv6地址范围从 0000:0000:0000:0000:0000:0000:0000:0000 至 ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff。 ​ 总数：能产生2的128次方个IP地址，其资源几乎是无穷的 ​ A类IP地址 192.168.0.1中 192.168.0代表网络地址，168.0.1代表主机地址 其中主机地址0与255不能用，可以ip池为256256256=16777216种 B类IP地址 192.168.0.1中 192.168.0代表网络地址，0.1代表主机地址 其中主机地址0与255不能用，可以ip池为256*256=65536种 C类IP地址 192.168.0.1中 192.168.0代表网络地址，1代表主机地址 其中主机地址0与255不能用，可以ip池为254种 D类IP地址用于多点广播 D类IP地址第一个字节”1110”开始，它是一个专门保留的地址。 它并不指向特定的网络地址，目前这一类地址被用在多点广播中 多点广播地址用来一次寻址一组计算机s地址范围224.0.0.1-239.255.255.254 E类IP地址 以”1111”开始，为将来留作保留 仅作为实验与开发使用 私有IP 在这么多网络IP中，国际规定中有一部分IP是用于我们的局域网使用，也就是属于私网IP，不在公网中使用，它们的范围是： 10.0.0.0 ~ 10.255.255.255 172.16.0.0 ~ 172.31.255.255 192.168.0.0 ~ 192.168.255.255 注意事项 IP地址127·0·0·1 ~ 127·255·255·255用于回路测试。 如：127·0·0·1 可以代表本机IP，用于http://127.0.0.1 就可以测试本机中配置的Web服务器 总结 根据使用IP的主机数，来划分各类IP段","permalink":"http://yoursite.com/2020/07/12/Python/network/UDP/ip-addr/","photos":[]},{"tags":[{"name":"Automation InterFace Test","slug":"Automation-InterFace-Test","permalink":"http://yoursite.com/tags/Automation-InterFace-Test/"}],"title":"接口自动化介绍","date":"2020/07/07","text":"导语合格的自动化测试工程师 -&gt; 接口自动化测试框架 入门条件 -&gt; 设计-开发-重构框架 进阶之路：接口基础 -&gt; 接口开发 -&gt; Unittest与接口测试结合 -&gt; 设计，开发框架 接口基础 HTTP接口熟悉 常见接口介绍 接口工具的使用 接口测试基础的面试 接口开发 使用Django，开发Get，Post请求 掌握Unittest与接口测试的结合 Unittest使用，断言，HTMLTestRunner，Case的管理，Request的引入使用 接口自动化测试框架从设计到开发 设计框架 根据框架，设计工具封装 基类封装 Debug 数据处理 回写测试结果 解决数据依赖 结果统计 邮件服务 发送报告 常见接口测试查缺补漏 操作数据库 操作cookie 接口基础面试 如何理解接口？ 前后端解耦，需要一个桥梁，负责前后端传输数据 接口测试和功能测试的区别 是功能测试的一种，功能测试的定义非常广泛 自动化属于功能测试，无论是接口还是自动化测试，都是模拟用户的操作 接口测试只是模拟用户发送数据，只是没有经过前端/客户端 自动化模拟用户操作，减去了手工操作，通过脚本去实现 在国内来说，技术能力可能会比功能测试好-含金量不同 常见的接口类型 Post，get，put，delete 接口是如何传递参数 Get请求?前方为URL地址，后方为消息主体/请求数据 Post请求是以WebFrom/表单的形式提交数据 如何测试一个接口？ Fiddler Postman soapUI Loadrunner Jmeter 工具只是一种实现方式而已 怎么简单怎么做（主要是效率） 模拟请求 为什么模拟请求 解耦很重要 并行开发很重要（效率第一） 使用Fiddler模拟请求 步骤：进入Composer工具栏，传入对应headers，body 使用fiddler模拟响应 进入AutoResponder工具栏，传入对应url，以及对应的response数据 搭建Django框架-测试环境开发web端是比较实用的 安装Django框架 在线安装：pip install Django ==指定版本 离线安装：python setup.py install 官网下载所需版本 解压，然后在cmd中，进入存放目录，输入命令 在终端输入命令：django-admin startproject HelloWorld（项目名可修改） ​ HelloWorld/根目录 项目的容器。它的名字对Django无关紧要。您可以将其重命名为您喜欢的任何名称。 manage.py 一个命令行实用程序，可让您以各种方式与该Django项目进行交互。您可以manage.py在 django-admin和manage.py中 阅读有关的所有详细信息 。 内部HelloWorld/目录 是项目的实际Python包。它的名称是Python包名称，您需要使用它来导入其中的任何内容（例如HelloWorld.urls）。 HelloWorld/init.py 一个空文件，告诉Python该目录应视为Python软件包。如果您是Python初学者，请在Python官方文档中阅读有关包的更多信息。 HelloWorld/settings.py 此Django项目的设置/配置。 Django设置将告诉您所有设置的工作方式。 HelloWorld/urls.py 此Django项目的URL声明；Django支持的网站的“目录”。您可以在URL调度程序中阅读有关URL的更多信息。 HelloWorld/asgi.py 与ASGI兼容的Web服务器为您的项目提供服务的入口点。有关更多详细信息，请参见如何使用ASGI进行部署。 HelloWorld/wsgi.py 与WSGI兼容的Web服务器为您的项目提供服务的入口点。有关更多详细信息，请参见如何使用WSGI进行部署 运行django 在根目录下，在终端中运行：python manage.py runserver 127.0.0.1:8000 在browser中，输入127.0.0.1:8000 创建工程应用app 创建新的app，使用系统模块manage 回到根目录，进入终端：python manage.py startapp name 原理 需要知道地址怎么到server的，访问urls.py中的方法，执行urlpatterns中的方法 创建的app中的views.py模块中，定义方法 在urls.py模块中，引入login方法，urlpatterns变量中，调用login方法 用户在敲下你的网址并回车，生成请求 请求传递到urls.py；Django去urlpatterns中匹配链接（Django会在匹配到的第一个就停下来） 一旦匹配成功，Django便会给出相应的view页面（该页面可以为一个Python的函数，或者基于view（Django内置的）的类），也就是用户看到的页面 若匹配失败，则出现错误的页面 一个工程项目，有很多个APP，每个APP都是工程组成的一部分 开发接口 框架 都是基于代码 框架就是把所有的代码进行整合，写的简洁一些，逻辑是没有改变的 无非是多个中间服务商，Apache，Nginx之类的 只需要知道url跳转到哪儿，在哪儿解析，Response什么信息 真正的API在views模块里面 注意事项： 代理端口需区分，否则会冲突 HTML中form表单，与from区分 HTML中路径需以/斜杠结尾 Unittest 创建一个类的时候，继承Unittest框架中TestCase类 内置方法 Command Describe setUp 每次方法之前执行 tearDown 每次方法之后执行 sefUpClass 类执行前执行 tearDownClass 类执行后执行 Test_XX 单词在前的case方法，缺少test则不会运行 断言–assert Command Describe assertEqual x == y ？None : z assertNotEqual x != y ？None : z assertTrue type(x) == True ? None : z 全局变量相比类变量更加快捷 命名方式：Globals()[‘变量名’] 解决依赖 跳过case执行： 语法糖：@unittest.skip(‘case’) 创建容器 创建语法 Suite = unittest.TestSuite() 添加case Suite.addTest(类名(casename)) 可添加多个case 执行容器 unittest.TextTestRunner().run(容器名) 结合HTMLTestRunner，生成TestReport 搜索Python3 HTMLTestRunner，最好是GitHub上面的 创建HTMLTestRunner.py，移至Python-&gt;lib目录 在case文件中，引入HTMLTestRunner模块 Unittest之面试 如何使用Python开发测试框架 语言：Python 接口：使用requests第三方库 管理case：unittest框架-断言-skipcase-suite容器-依赖关系 报告：HTMLTestRunner 数据管理：Excel，MySql Case执行：持续集成或批处理文件 建议 Unittest中，case是按照ascll码比较，进行升序执行 尽量减少依赖执行 GET请求URL与Data分离传参，无响应 Globals()[‘变量’] = data 下个case调用会报错 Mock服务 以代码来实现自动响应请求的功能，模拟返回数据 引入mockimport unittest.mock 使用方法mock_name = mock.Mock(return_value=模拟数据) 请求方法 = mock_name(改变返回模拟数据) 示例mock_data = mock.Mock(return_value=response_data) method = mock_data res = method(request_data, url, method) 重构封装mock封装mock-method from unittest import mock def mock_test(mock_method, url, method, request_data=None, response_data=None): if response_data == None: res = mock_method(url, method, response_data) return res else: mock_data = mock.Mock(return_value=response_data) mock_method = mock_data res = mock_method(request_data, url, method) return res 设计接口自动化测试框架 Case过多，避免繁琐操作 不排除某些需求，设定特殊的值，而手动添加 编写接口测试用例 需要考虑的点 接口地址 请求数据 接口类型 预期结果 Header - cookie 数据依赖–接口自动化的难点 操作Excel 定义一个类 重构封装excel函数 封装request_data, header 封装调用类 封装request类 对response_data，进行格式化 Json.dumps(response, ensure_ascii=False, intent=2, sort_keys=True) 使用ensure_ascii的注意事项： json_dumps(dict)时，如果dict包含有汉字，一定加上 ensure_ascii=False。否则按参数默认值True，意思是保证dumps之后的结果里所有的字符都能够被 ascii 表示，汉字在ascii的字符集里面，因此经过dumps以后的str里，汉字会变成对应的unicode。 写入excel测试结果 判断接口返回状态： 接口是否通畅 调用status_code方法： 引入jsonpath_rw 第三方库 从第三方库中引入jsonpath，parse 方法 B = {'foo': [{'baz': 'news'}, {'baz': 'music'}]} 赋予规则 A = parse(‘foo[*].data’) 解析数据 C = A.find(b) 使用列表推导式，获取数据 Data = [match.value for match in C][0] index get content","permalink":"http://yoursite.com/2020/07/07/Python/interface/interface_automation/","photos":[]},{"tags":[{"name":"InterFace Test","slug":"InterFace-Test","permalink":"http://yoursite.com/tags/InterFace-Test/"}],"title":"接口自动化测试","date":"2020/07/07","text":"为什么要做接口测试？ 在日常前后端开发中，他们之间为什么是独立完成的？ 接口的由来：连接前后端，以及移动端 不同端的工作进度不一样，需要对最开始出来的接口，进行接口测试 接口测试带来了哪些好处？ 比如调用第三方的接口，淘宝，银行，支付宝…，那么我们就要进行接口测试，以及验证数据 节约时间，缩短项目时间 提高工作效率 提高系统的稳定性 什么是接口？ http请求 -&gt; 接口 ，不断地操作系统 -&gt; 系统不断的去服务端，第三方服务调用接口 接口的种类 内部接口 外部接口 我们测试的接口，接收测试http，tcp请求 Get，Post请求是日常中，使用较多的接口类型 Get，Post的区别 理论上GET请求数据长度没有限制的，真正起到限制的是浏览器对其长度进行了限制。 POST请求理论上也是不限制大小的，真正对其大小进行限制的是服务器的处理程序能力 参数提交方式 Post请求的数据是WebForm里面的，以表单的形式提交 Get请求的数据是在地址栏中，进行请求的 接口地址以？分隔开，后面就是数据，以&amp;连接符连接 请求数据大小 Get请求的数据相比Post是较少的 GET请求的URL长度根据不同的浏览器，会有不同的字节限制 Browser Length IE URL &lt;= 2083 Path &lt;= 2048 Firefox 65536 Safari 80000以上 Opera 190000以上 超出最大抓取/索引字节长度 会报414 Chrome 8182 Apache Sever 8192 iis 16384 Perl Http:Daemon 至少8000 安全性 Get请求的用户信息在URL栏中，安全性相比Post表单请求，安全性低 接口测试流程 一个简单的事情，需要认真，有条理的去执行 可能当时没有出错，谁又能保证以后不出错，尽量做好万全的准备 熟悉接口测试流程的好处： 以备面试所需 熟悉接下来的流程 掌握基础知识 面试时，通常基础知识问答较多 测试流程的重点：设计用例 理清思路，避免漏测（不可进行随机测试） 提高测试效率 跟进测试进度 证明进行过测试（一种凭证） 跟进重复性工作 黑盒：需求文档 接口测试：需要开发提供的API文档 接口测试用例设计 接口测试工具 提高工作效率，不管是否上线 使用工具的目的 举例： 10分钟测试10个get接口，使用不熟悉的loadrunner录制脚本，对接口进行参数化，然后查看日志结果。此时就不宜使用臃肿的测试工具 如何选择接口测试工具 时间，简易程度，业务复杂度，测试员能力 接口测试工具分类 Tools Purpose Httpwatch HTTP 仅支持IE，firefox浏览器，看数据比较麻烦 Wireshark TCP/HTTP 功能齐全，经过PC端的请求http/tcp，都能抓取到，所以数据量大，查看比较麻烦 Fiddler HTTP 可直接抓取http请求，小巧，功能完善，启动快捷，代理方便 测试接口 Loadrunner 第一印象：是一个性能测试工具 功能强大，可以进行接口测试 所有性能测试都是基于http请求的 把接口按照http请求的格式，进行测试 通过两个函数 Fidder 轻量级抓包兼测试工具 SoapUI 比较强大的测试工具，可以做接口，也可以做自动化 功能也比较齐全，返回的乱码可进行转换，打开接口文档对比 Jmeter 不仅仅是性能测试，可以使用http请求进行接口测试 Postman 测试中常用选择 接口测试工具-三剑客 Postman 导入body中的数据一定要仔细核对 如何自动反复测试带header的Post请求，并且针对线上，测试环境同一个数据进行测试。并对实际结果作比较，作为测试结果 各种类型接口测试 按需求添加cookie，header 操作记录清晰 多接口同时测试 方便回归 提高技能水平 Fiddler 简介： 是一款http协议代理调试工具，能够记录并检查电脑与互联网之间所有的http协议，设置断点，查看所有”进出”Fiddler的数据 工作原理： 运用步骤： 查看接口请求方式 查看请求数据，响应数据 接口返回状态 设置代理（抓取移动端接口请求） 查看header，cookie 设置断点 添加筛选 接口测试 Python自开发 工具没法满足需求时 安全性 业务限制 逻辑清晰-&gt;表达，理解能力强-&gt;看懂代码-&gt;会写代码-&gt;会改代码 测试流程 通过urllib urllib2 扩展库 定义接口地址 定义请求数据 整理请求数据 按照格式拼接，按照规定格式发送server 发送数据并获取返回结果","permalink":"http://yoursite.com/2020/07/07/Python/interface/interface_test_basic/","photos":[]},{"tags":[{"name":"Basics","slug":"Basics","permalink":"http://yoursite.com/tags/Basics/"},{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"}],"title":"Git基础","date":"2020/06/30","text":"一．Git基础1．Git介绍 Git是目前世界上最先进的分布式版本控制系统 版本控制系统： 记录每个迭代版本的信息 版本号 文件名 操作用户 日志 修改时间 1 Test.py well 修改标题 2020/04/18/17:15 2 Test.py jack 优化性能 2020/04/18/18:00 3 Test.py esion 修复bug 2020/04/18/19:15 4 Test.py jack 功能更新 2020/04/18/20:15 2．Git与Github2.1.两者区别 Git 是一个分布式版本控制系统，简单来说它就是一个软件，用于记录一个或多干个文件迭代信息，以便将来查阅某个版本修订情况的软件 Github 是一个为用户提供Git服务的网站，简单说就是一个可以存放代码的地方，也可以是其他东西，Github除了提供管理Git的Web界面外，还提供了订阅，关注，讨论组，在线编辑器等丰富的功能。 Github被称之为全球最大的基友网站。 3．Git安装3.1.下载安装包，进行安装3.2.默认安装即可，最后的view就不要勾选了 二．Git的使用1．本地仓库1.1. 工作流程 Git本地操作的三个区域-&gt;示意图： 工作流程-&gt;示意图： 1.2.本地仓库操作 什么是仓库呢？仓库又名为版本库，英文名repository，可以简单的理解成是一个目录，用于存放代码，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改，删除等变更状态Git都能跟踪到。 在安装好后首次使用需要进行全局配置,进入Git bash here 进入Git命令行窗口，输入以下命令： Config --global user.name “xxx” 配置用户名 Config --global user.email “xxx” 配置邮箱 在多人协作项目中，用于身份的甄别 创建仓库 当我们需要让Git去管理某个新项目/已存在的项目，就需要创建仓库了。注意，创建仓库的目录不一定要求是空目录，选择一个非空目录也是可以的，但是不建议在现有的项目上来学习Git。 创建空目录 Mkdir file_name 进入目录 Git仓库初始化 Git init 生成git的隐式目录 1.3. \\Git常用指令** 查看当前状态 git status 添加至缓存区 添加一个文件 git add file_name添加多个文件 git add file_1 file_2 file_3添加全部文件 git add . 移除缓存区文件 移出一个文件 git rm -r --cached file_name移出多个文件 git rm -r --cached .移出全部文件 git rm -r --cached .idea 提交至版本库 git commit -m “更新内容描述” 1.4.版本回退 查看版本，确定需要回到的时间点 版本号仅会显示当前，以及当前以前的版本信息 commit bc22b1d47fe55db8901129da1f6c74fdbbddd5b8 (HEAD -&gt; master) 目前版本 获得详细版本状态信息 git log 获得版本提交编号，与注释内容，一行显示 git log --pretty=oneline 回退 git reset --hard 版本号 回退至历史版本后，在回退到当前，则需要查看历史操作，得到想要commit_id git reflog 小结： 回退各个时间段的版本，需获得commit id，进行 git reset –hard commit_id 再次回到当前版本，需要进行reflog直接获得历史操作，以及获得7位commit_id commit_id可以不用输入完整，git会自动识别，但也不能太少，可能会出现重复，至少写前4位字符 1.5. 忽略文件 应用场景 在项目目录下有很多万年不变的文件目录，例如css，js，images等，或者还有一些目录即便有改动，我们也不想让其提交到线上仓库。 实现流程 忽略文件需要新建一个名为.gitignore的规则文件，该文件用于声明忽略文件或不忽略文件的规则，规则对当前目录与子目录生效 注意：该文件没有文件名，没办法直接在windows目录下创建，可以通过命令行创建。 常见规则 过滤整个子目录 /mtk 过滤当前目录与子目录中.zip文件 *.zip 过滤具体文件 /mtk/.doc 不过滤该文件 ! Test.py 创建过滤文件 touch .gitignore 2.远程仓库线上仓库的操作学习以GIthub为例 线上仓库创建，两种常规使用方式 基于http协议 创建空目录，名称就称为WellHome Mkdir WellHome 使用clone指令，克隆线上仓库到本地 git clone source.git 仓库基本操作（提交暂存区，提交本地仓库，提交线上仓库，拉取线上仓库） 提交当线上仓库的指令 git push 若提交后，返回requested 403 因为需要鉴权，则需要权限 需要修改配置文件，在请求url 中添加用户名与密码 拉取线上仓库 git pull 在每天工作的第一件事 git pull 每天下班时做的最后一件事 git push 基于SSH协议 需安装OpenSSH（推荐使用） Https与SSH协议区别：该方式与前面https方式相比，至少影响github对于用户的身份鉴权方式，对于git 的具体操作（如提交本地，添加注释，提交远程等操作）没有任何影响 生成公私钥对指令（需自行安装OpenSSH） ssh-keygen -t rsa -C “注册邮箱”，直接回车继续，返回提示生成的公私钥对的路径 进入提示目录下，找到公钥文件，添加到github中，并编写title clone 仓库 git clone git_ssh地址 — 返回提示yes or no 输入yes即可 分支管理 分支示意图 ​ 在版本回退的操作中，每次提交后都会有记录，Git把它们串成时间线，形成类型时间轴的存在，这个时间轴就是一个分支，我们称为master分支（主分支）。​ 在多人团队协作开发的项目中，一个分支是无法满足多人同时开发的需求的，并且分支上工作并不影响他人正常使用，会更加安全，Git鼓励开发者使用分支去完成开发任务。 分支相关指令 查看分支 git branch 创建分支 git branch 名称 切换分支 git checkout 名称 删除分支 git branch -d 名称 合并分支 git merge 被合并名称 冲突的产生与解决 当线上与本地仓内容不一致时，git会检测出来，并提示让你先pullPull后，git会自动合并代码在改文件中。 解决方法 与变更文件同事沟通，再进行操作 提醒 上班后同步线上代码 图形化管理工具 Github for Desktop Github出品，功能完善，界面简洁，使用方便。 Source tree 老牌Git GUI管理工具，也号称最好用的管理工具，功能丰富，基本操作和高级操作都非常流畅，适合初学者。 TortoiseGit 对于熟悉SVN的开发人员来说，这个小乌龟是非常友善了，简称tgit，中文名海龟Git， 它与前辈TortoiseSVN都是非常优秀的开源版本控制客户端软件。","permalink":"http://yoursite.com/2020/06/30/Git/basic/","photos":[]},{"tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/tags/Scrapy/"},{"name":"Lagou","slug":"Lagou","permalink":"http://yoursite.com/tags/Lagou/"}],"title":"settings组件分析","date":"2020/06/28","text":"COOKIES_ENABLED 进入 scrapy 包 -&gt; downloadermiddlewares 子包 -&gt; cookies模块中 class CookiesMiddleware: \"\"\"This middleware enables working with sites that need cookies\"\"\" # 使用该中间件可以处理需要Cookie的网站 def __init__(self, debug=False): self.jars = defaultdict(CookieJar) self.debug = debug @classmethod def from_crawler(cls, crawler): # 这是一个组件，作用于所有的scrapy Request # 通过这个组件，提取前一个Request中的cookie，并加入下一个Request cookie中去 if not crawler.settings.getbool('COOKIES_ENABLED'): raise NotConfigured return cls(crawler.settings.getbool('COOKIES_DEBUG')) def process_request(self, request, spider): if request.meta.get('dont_merge_cookies', False): return USER_AGENT设置全局USER_AGENT，与DOWNLOADERMIDDLEWARES 配合 USER_AGENT = &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36&#39; DOWNLOADER_MIDDLEWARES 赋予每个Scrapy Request USER-AGENT DOWNLOADER_MIDDLEWARES = { # &#39;ArticleSpider.middlewares.ArticlespiderDownloaderMiddleware&#39;: 543, &#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;: 2 }","permalink":"http://yoursite.com/2020/06/28/Python/crawler/lagou/Settings/","photos":[]},{"tags":[{"name":"Basics","slug":"Basics","permalink":"http://yoursite.com/tags/Basics/"}],"title":"MySQL基础","date":"2020/06/28","text":"查看库，表，内容 查看MySQL版本号 mysql -V or mysql --version 查看当前所有库 SHOW databases; 进入数据库 USE db_name; 查看当前所有表 SHOW TABLES; SHOW TABLES from db_name; # 查看指定库的表 查看表内容 SELECT * FROM table_name; 创建表，以及字段 CREATE TABLE table_name( Id int(10), # 字段|类型 Name varchar(20)); # 字段|类型 查看所在库 SELECT database(); 查看表结构 DESC table_name; 插入数据 INSERT INTO table_name(id, name) value(1, “mack”); 修改数据 UPDATE table_name SET name=”jack” WHERE id=1; 删除表数据 DELETE FROM table_name WHERE id=1; 基础查询 语法： SELECT field FROM table_name; 特点：查询列表可以是：表中的字段，常量值，表达式，函数 查询表中的单个字段 SELECT field1 FROM table_name; 查询表中多个字段 SELECT field1, field2 from table_name; 查询表中所有字段 SELECT * FROM table_name; 区分关键字与字段 字段名前后加入 ` 符号 查询常量值 SELECT 100; SELECT ‘join’; 查询表达式 SELECT 100*100; 查询函 SELECT version(); 调用version函数返回版本号 赋予别名，改变字段名称 利于理解 如果查询的字段存在重名，使用别名可以进行区分方法一 Select version() as one; 方法二 ``` select version() one; select version() “one pic”; ``` +号的作用 加法运算 SELECT ‘join’ + 100; 输出结果：100 其中一方为字符串，则转换类型，转换失败则置0处理 若一方为Null，则结果为Null DISTINCT-去重 SELECT DICTINCT 字段名 FROM 表名; CONCAT-连接查询字段 SELECT CONCAT(field1, ‘,’ field2) as 列名 FROM table_name; IFNULL-判空函数 SELECT IFNULL(字段名, 0) FROM table_name; 第一个参数为Null，则替换为第二个参数 条件查询 语法 Select 查询字段 from 表名 where 筛选条件; 按条件表达式筛选条件运算符：&gt; &lt; &gt;= &lt;= != &lt;&gt; = 案例1：查询工资&gt;12000的员工信息 SELECT * FROM table_name W salary > 12000; 案例2：查询部分编号!=90的员工名和部门编号 SELECT last_name, first_name, department_id FROM employees WHERE department &lt;> 90; 按逻辑表达式筛选逻辑运算符：And or not 案例1：查询工资在10000到20000之间的员工名，工资，奖金信息 SELECT last_name, salary, commission_pct FROM employees WHERE salary > 10000 AND salary &lt; 20000; 案例2：查询部门编号不是在90到110之间，或者工资高于15000的员工信息 SELECT * FROM employees WHERE department_id > 110 OR department_id &lt; 90 OR salary > 15000; 因为 &gt;110 与 &lt;90条件不冲突，以及salary字段&gt;15000最后才执行 模糊查询关键字：like，between and，in，is null like：一般和通配符配合使用 案例1：查询员工名中包含字符a的员工信息 select * From employees Where last_name like ‘%a%’; 字符串必须使用单引号，%代表通配符，任意字符，包含0个字符 扩展知识Mysql报错1366 解决方案：修改此表的字符集 分别操作：查看表中每个字段的字符集，修改表中的字段的字符集 查看表结构：show full columns from 表名; 改变表结构：alter table 表名 convert to character set utf8; 创建用户和授权在mysql8.0创建用户和授权和之前不太一样了，其实严格上来讲，也不能说是不一样,只能说是更严格,mysql8.0需要先创建用户和设置密码,然后才能授权. 先创建一个用户 create user 'tone'@'%' identified by '123123'; 进行授权 grant all privileges on *.* to 'tone'@'%' with grant option; 如果还是用原来5.7的那种方式，会报错误： grant all privileges on *.* to 'tome'@'%' identified by '123123'; 抛出错误： ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;identified by &#39;123123&#39;&#39; at line 1 MySQL8.0 的远程链接MySQL 安装完成后只支持 localhost 访问，我们必须设置一下才可以远程访问，另外还有一些 MySQL 8.0 连接时的一些问题. 登录MySQL mysql -u root -p 输入您的密码，选择 mysql 数据库 use mysql; 因为 mysql 数据库中存储了用户信息的 user 表，在 mysql 数据库的 user 表中查看当前 root 用户的相关信息 select host, user, authentication_string, plugin from user; 执行完上面的命令后会显示一个表格，查看表格中 root 用户的 host，默认应该显示的 localhost，只支持本地访问，不允许远程访问。授权 root 用户的所有权限并设置远程访问 GRANT ALL ON *.* TO 'root'@'%'; GRANT ALL ON 表示所有权限，% 表示通配所有 host，可以访问远程。 ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '你自己的密码'; 刷新权限所有操作后，应执行 flush privileges; 查看 root 用户的 host use mysql; select host, user, authentication_string, plugin from user; 发现 root 用户的 host 已经变成 %，说明我们的修改已经成功，可以远程访问了。 访问数据库 远程访问数据库的GUI工具比较多如：Navicat、SQLyog、MySQL workbench 等，我这里使用 Navicat 输入访问的 host 和密码，报 2059 错误，这是因为 MySql8.0 版本 和 5.7 的加密规则不一样，而现在的可视化工具只支持旧的加密方式。 出现这个原因是mysql8 之前的版本中加密规则是mysql_native_password,而在mysql8之后,加密规则是caching_sha2_password.解决问题方法有两种,一种是升级navicat驱动,一种是把mysql用户登录密码加密规则还原成mysql_native_password. ​ 这里采用第二种方式 ： ​ 修改加密规则: ALTER USER 'root'@'%' IDENTIFIED BY 'password' PASSWORD EXPIRE NEVER; password 为你当前密码。 ​ 更新 root 用户密码: ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY 'password'; password 为你新设置的密码。 ​ 刷新权限: FLUSH PRIVILEGES; ​ OK，设置完成，再次使用 Navicat 连接数据库","permalink":"http://yoursite.com/2020/06/28/MySQL/basic/","photos":[]},{"tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/tags/Scrapy/"},{"name":"Lagou","slug":"Lagou","permalink":"http://yoursite.com/tags/Lagou/"}],"title":"crawlspider源码分析","date":"2020/06/28","text":"查看可用 scrapy 模板 进入项目目录，输入以下命令scrapy genspider --list 可用模板列表Available templates: basic # 默认模板 crawl # 全站爬取 csvfeed # CSV 源模板 xmlfeed # XML 源模板 basic crawlCrawlSpider 是爬取那些具有一定规则网站的常用的爬虫，它基于Spider并有一些独特属性 基于 Spider 类，进一步封装 rules: 是Rule 对象的集合，用于匹配目标网站并排除干扰 parse_start_url: 用于爬取起始响应，必须要返回item，Request中的一个 _parse_response: 是CrawlSpider 的核心方法 创建CrawlSpider - > scrapy genspider -t crawl spider_name spider_url 注意：在CrawlSpider中，不可以进行重构 parse 方法，因为它已经被CrawlSpider占用，可以使用parse_strat_url方法替代 csvfeed xmlfeed 添加 source root -&gt; settings 配置中# 添加所属目录 -> PythonPath 中 sys.path.insert(0, os.path.dirname(os.path.abspath(__file__))) CrawlSpider 源码逻辑概述class LagouSpider(CrawlSpider): name = 'lagou' allowed_domains = ['www.lagou.com'] start_urls = ['http://www.lagou.com/'] rules = ( # 参数为可迭代对象 # rule 实例，LinkExtractor (链接提取器) 实例 # 可以更改域名, 一般大型网站都有负载均衡处理, 在某个城市进行CDN, 每个城市的URL/IP地址都是不一样的, 获取到多个城市的URL后,可以进行随机IP访问，减少了IP被检测的概率 Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True), ) def parse_job(self, response): \"\"\"解析拉勾网职位信息\"\"\" item = {} #item['domain_id'] = response.xpath('//input[@id=\"sid\"]/@value').get() #item['name'] = response.xpath('//div[@id=\"name\"]').get() #item['description'] = response.xpath('//div[@id=\"description\"]').get() return item class CrawlSpider(Spider): rules = () # 在 CrawlSpider 初始化时, 调用 compile_rules 方法 def __init__(self, *a, **kw): super(CrawlSpider, self).__init__(*a, **kw) self._compile_rules() def parse(self, response): return self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True) def parse_start_url(self, response): return [] def process_results(self, response, results): return results def _build_request(self, rule_index, link): return Request( url=link.url, callback=self._callback, errback=self._errback, meta=dict(rule=rule_index, link_text=link.text), ) def _requests_to_follow(self, response): \"\"\"要求遵循\"\"\" # 判断是否为 HTMLResponse if not isinstance(response, HtmlResponse): return # 新建一个 set 类型局部变量, 对 response 中的 url 进行去重 seen = set() # 通过 enumerate 把 _rules 改变为一个可迭代的对象 for rule_index, rule in enumerate(self._rules): # 把 response 传递给 link_extractor 类 extract_links 方法, 提取出具体的 link links = [lnk for lnk in rule.link_extractor.extract_links(response) if lnk not in seen] # 自定义 process_links 方法, 传递给 Rule 类, 抽取出 link 添加至 set 中 for link in rule.process_links(links): seen.add(link) request = self._build_request(rule_index, link) yield rule._process_request(request, response) def _callback(self, response): rule = self._rules[response.meta['rule']] return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow) def _errback(self, failure): rule = self._rules[failure.request.meta['rule']] return self._handle_failure(failure, rule.errback) def _parse_response(self, response, callback, cb_kwargs, follow=True): \"\"\" CrawlSpider 中核心方法 :param response: :param callback: 回调方法名 self.parse_start_url :param cb_kwargs: 获取 parse_start_url 返回的参数 :param follow: :return: \"\"\" if callback: cb_res = callback(response, **cb_kwargs) or () # 交由 process_results 方法 cb_res = self.process_results(response, cb_res) # 对返回结果，进行迭代(抛出 Item，交给 Scrapy 进行传递) for request_or_item in iterate_spider_output(cb_res): yield request_or_item # CrawlSpider 核心中的核心, 默认进行跟随链接, 改变follow/_follow_links bool 值, 决定是否跟随 if follow and self._follow_links: for request_or_item in self._requests_to_follow(response): yield request_or_item def _handle_failure(self, failure, errback): if errback: results = errback(failure) or () for request_or_item in iterate_spider_output(results): yield request_or_item def _compile_rules(self): \"\"\"制定规则\"\"\" # 生成实例变量 self._rules = [] for rule in self.rules: # 浅拷贝 rule 值 self._rules.append(copy.copy(rule)) self._rules[-1]._compile(self) @classmethod def from_crawler(cls, crawler, *args, **kwargs): spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs) # 获取 settings 中 'CRAWLSPIDER_FOLLOW_LINKS' 参数 (需自定义)，若没定义则取默认值 True # 若设置 'CRAWLSPIDER_FOLLOW_LINKS' 参数为False，rules 则会失效 spider._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True) return spider Rule 类class Rule: def __init__(self, link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None, errback=None): self.link_extractor = link_extractor or _default_link_extractor self.callback = callback self.errback = errback self.cb_kwargs = cb_kwargs or {} self.process_links = process_links or _identity self.process_request = process_request or _identity_process_request self.process_request_argcount = None self.follow = follow if follow is not None else not callback link_extractor 一个具体的 extractor 类，用于完成 url 的抽取 callback 回调函数 cb_kwargs 传递给 link_extractor 的参数 follow 满足 rule 的 url 是否进行跟踪 process_links 可以自定义的预处理方法 参数类型 -&gt; function process_request 对 request 进行处理 参数类型 -&gt; function LinkExtractor 类class LxmlLinkExtractor(FilteringLinkExtractor): def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), tags=('a', 'area'), attrs=('href',), canonicalize=False, unique=True, process_value=None, deny_extensions=None, restrict_css=(), strip=True, restrict_text=None): tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs)) lx = LxmlParserLinkExtractor( tag=lambda x: x in tags, attr=lambda x: x in attrs, unique=unique, process=process_value, strip=strip, canonicalized=canonicalize ) super(LxmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny, allow_domains=allow_domains, deny_domains=deny_domains, restrict_xpaths=restrict_xpaths, restrict_css=restrict_css, canonicalize=canonicalize, deny_extensions=deny_extensions, restrict_text=restrict_text) def extract_links(self, response): \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the specified :class:`response &lt;scrapy.http.Response>`. Only links that match the settings passed to the ``__init__`` method of the link extractor are returned. Duplicate links are omitted. \"\"\" base_url = get_base_url(response) if self.restrict_xpaths: # 编译 xpath 参数 docs = [subdoc for x in self.restrict_xpaths for subdoc in response.xpath(x)] else: docs = [response.selector] all_links = [] for doc in docs: links = self._extract_links(doc, response.url, response.encoding, base_url) all_links.extend(self._process_links(links)) return unique_list(all_links) allow 正则提取，设定的 rules allow 参数，进行处理 deny 正则提取，设定的 rules allow 参数，不处理 allow_domains 设定域名下的 url，进行处理 deny_domains 设定域名下的 url，不处理 restrict_xpaths 指定 xpath 规则，进行提取 tags 默认提取标签 a, area attrs (attribute) 默认提取 href 属性中的值 restrict_css 指定 css 规则，进行提取 (最终都会被转换为 xpath 进行处理) Ps：css 语法是 HTML 支持的， XML 是不支持的，最早 Xpath 是用来提取 XML 的 父类 FilteringLinkExtractorclass FilteringLinkExtractor: _csstranslator = HTMLTranslator() def __new__(cls, *args, **kwargs): from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor if (issubclass(cls, FilteringLinkExtractor) and not issubclass(cls, LxmlLinkExtractor)): warn('scrapy.linkextractors.FilteringLinkExtractor is deprecated, ' 'please use scrapy.linkextractors.LinkExtractor instead', ScrapyDeprecationWarning, stacklevel=2) return super(FilteringLinkExtractor, cls).__new__(cls) def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains, restrict_xpaths, canonicalize, deny_extensions, restrict_css, restrict_text): self.link_extractor = link_extractor self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)] self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)] self.allow_domains = set(arg_to_iter(allow_domains)) self.deny_domains = set(arg_to_iter(deny_domains)) self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths)) self.restrict_xpaths += tuple(map(self._csstranslator.css_to_xpath, arg_to_iter(restrict_css))) self.canonicalize = canonicalize if deny_extensions is None: deny_extensions = IGNORED_EXTENSIONS self.deny_extensions = {'.' + e for e in arg_to_iter(deny_extensions)} self.restrict_text = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(restrict_text)] def _link_allowed(self, link): if not _is_valid_url(link.url): return False if self.allow_res and not _matches(link.url, self.allow_res): return False if self.deny_res and _matches(link.url, self.deny_res): return False parsed_url = urlparse(link.url) if self.allow_domains and not url_is_from_any_domain(parsed_url, self.allow_domains): return False if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains): return False if self.deny_extensions and url_has_any_extension(parsed_url, self.deny_extensions): return False if self.restrict_text and not _matches(link.text, self.restrict_text): return False return True def matches(self, url): if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains): return False if self.deny_domains and url_is_from_any_domain(url, self.deny_domains): return False allowed = (regex.search(url) for regex in self.allow_res) if self.allow_res else [True] denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else [] return any(allowed) and not any(denied) def _process_links(self, links): links = [x for x in links if self._link_allowed(x)] if self.canonicalize: for link in links: link.url = canonicalize_url(link.url) links = self.link_extractor._process_links(links) return links def _extract_links(self, *args, **kwargs): return self.link_extractor._extract_links(*args, **kwargs) # Top-level imports from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor 引用 HTMLTranslator 类class HTMLTranslator(TranslatorMixin, OriginalHTMLTranslator): @lru_cache(maxsize=256) def css_to_xpath(self, css, prefix='descendant-or-self::'): return super(HTMLTranslator, self).css_to_xpath(css, prefix) css 转换 xpath","permalink":"http://yoursite.com/2020/06/28/Python/crawler/lagou/CrawlSpider-code/","photos":[]},{"tags":[{"name":"Lagou","slug":"Lagou","permalink":"http://yoursite.com/tags/Lagou/"},{"name":"Practice","slug":"Practice","permalink":"http://yoursite.com/tags/Practice/"}],"title":"拉钩网MySQL表结构设计","date":"2020/06/28","text":"MySQL 表结构设计 分析网站数据，制定合理的表结构","permalink":"http://yoursite.com/2020/06/28/Python/crawler/lagou/devise-mysql-table/","photos":[]}],"categories":[],"tags":[{"name":"Learn","slug":"Learn","permalink":"http://yoursite.com/tags/Learn/"},{"name":"Selenium","slug":"Selenium","permalink":"http://yoursite.com/tags/Selenium/"},{"name":"Basics","slug":"Basics","permalink":"http://yoursite.com/tags/Basics/"},{"name":"UDP","slug":"UDP","permalink":"http://yoursite.com/tags/UDP/"},{"name":"Socket","slug":"Socket","permalink":"http://yoursite.com/tags/Socket/"},{"name":"Port","slug":"Port","permalink":"http://yoursite.com/tags/Port/"},{"name":"Automation InterFace Test","slug":"Automation-InterFace-Test","permalink":"http://yoursite.com/tags/Automation-InterFace-Test/"},{"name":"InterFace Test","slug":"InterFace-Test","permalink":"http://yoursite.com/tags/InterFace-Test/"},{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/tags/Scrapy/"},{"name":"Lagou","slug":"Lagou","permalink":"http://yoursite.com/tags/Lagou/"},{"name":"Practice","slug":"Practice","permalink":"http://yoursite.com/tags/Practice/"}]}